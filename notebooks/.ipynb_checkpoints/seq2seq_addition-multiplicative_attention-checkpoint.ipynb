{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alignment(gluon.HybridBlock):\n",
    "    def __init__(self, n_hidden, **args):\n",
    "        super(alignment, self).__init__(**args)\n",
    "        with self.name_scope():\n",
    "            self.weight = self.params.get('weight', shape = (n_hidden, n_hidden), allow_deferred_init = True)\n",
    "        \n",
    "    def hybrid_forward(self, F, inputs, output, weight):\n",
    "        _s = F.dot(inputs, weight)\n",
    "        return gemm2(_s, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, enc_layer, dec_layer, ctx, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_layer = enc_layer\n",
    "        self.ctx = ctx\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            # we cannot apply dense layer for attention:\n",
    "            # Instead create parameter of size n_hidden * n_hidden\n",
    "            self.alignment = alignment(n_hidden)\n",
    "            self.attn_weight = nn.Dense(self.in_seq_len, in_units = self.in_seq_len)            \n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs, length = self.in_seq_len, merge_outputs = True)     \n",
    "        for i in range(self.out_seq_len):\n",
    "            # For each time step, caclculate context for attention\n",
    "            # Use enout(batch_size * in_seq_len * n_hidden)\n",
    "            _n_h = next_h.expand_dims(axis = 2)       \n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            score_i = self.alignment(enout, _n_h)\n",
    "            # Create attention weight: alpha_1, ... alpha_(in_seq_len)\n",
    "            alpha_i = nd.softmax(self.attn_weight(score_i))\n",
    "            alpha_expand = alpha_i.expand_dims(2)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout)\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(outputs[:, i, :], context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculate(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        # No label when evaluating new example. So try to put the result of the previous time step\n",
    "        alpha = []\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            _n_h = next_h.expand_dims(axis = 2)\n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            score_i = self.alignment(enout, _n_h)\n",
    "            alpha_i = nd.softmax(self.attn_weight(score_i))\n",
    "            # alpha:(n_batch * in_seq_len) -> Expand alpha to (n_batch * in_seq_len * n_hidden)\n",
    "            alpha_expand = alpha_i.expand_dims(2)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout)\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(deout, context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "            alpha.append(alpha_i.asnumpy())\n",
    "        return ret_seq.strip('E').strip(), np.squeeze(np.array(alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14, 2, 1, ctx)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculator30_ (\n",
       "  Parameter calculator30_lstm0_i2h_weight (shape=(1200, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm0_h2h_weight (shape=(1200, 300), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm0_i2h_bias (shape=(1200,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm0_h2h_bias (shape=(1200,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm1_i2h_weight (shape=(1200, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm1_h2h_weight (shape=(1200, 300), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm1_i2h_bias (shape=(1200,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_lstm1_h2h_bias (shape=(1200,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_alignment0_weight (shape=(300, 300), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_dense0_weight (shape=(9, 9), dtype=float32)\n",
       "  Parameter calculator30_dense0_bias (shape=(9,), dtype=float32)\n",
       "  Parameter calculator30_batchnorm0_gamma (shape=(0,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_batchnorm0_beta (shape=(0,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_batchnorm0_running_mean (shape=(0,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_batchnorm0_running_var (shape=(0,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter calculator30_dense1_weight (shape=(14, 0), dtype=float32)\n",
       "  Parameter calculator30_dense1_bias (shape=(14,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTMCell(None -> 1200)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (alignment): alignment(\n",
      "  \n",
      "  )\n",
      "  (attn_weight): Dense(9 -> 9, linear)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 2+60 = 769(62) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 4+43 = 440(47) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 22+196 = 101(218) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 388+5 = 1011(393) 0 attn_shape = (4, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 9+37 = 1027(46) 0 attn_shape = (4, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 16+601 = 769(617) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 13+814 = 108(827) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 7+476 = 1010(483) 0 attn_shape = (4, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 480+394 = 1100(874) 0 attn_shape = (4, 9)\n",
      "cov = \n",
      "[-0.06424443  0.05109791 -0.09911604 -0.07876181 -0.02678044]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 4+6 = 100(10) 0 attn_shape = (3, 9)\n",
      "Epoch 0. Train Loss: 1.0931786, Test Loss : 1.0792301\n",
      "Epoch 1. Train Loss: 0.989266, Test Loss : 0.8995689\n",
      "Epoch 2. Train Loss: 0.84161013, Test Loss : 0.78843933\n",
      "Epoch 3. Train Loss: 0.7105453, Test Loss : 0.6585453\n",
      "Epoch 4. Train Loss: 0.5805249, Test Loss : 0.5184509\n",
      "Epoch 5. Train Loss: 0.44655514, Test Loss : 0.4015231\n",
      "Epoch 6. Train Loss: 0.32555726, Test Loss : 0.3077426\n",
      "Epoch 7. Train Loss: 0.23895559, Test Loss : 0.30961767\n",
      "Epoch 8. Train Loss: 0.17944847, Test Loss : 0.1817147\n",
      "Epoch 9. Train Loss: 0.13825509, Test Loss : 0.15970074\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 1+1 = 36(2) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 3+8 = 100(11) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 93+7 = 1000(100) 0 attn_shape = (4, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 54+43 = 97(97) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 923+0 = 923(923) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 4+1 = 67(5) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 83+85 = 148(168) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 162+3 = 166(165) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 7+32 = 49(39) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.0201302   0.01785166 -0.10651072 -0.10960359 -0.05454021]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 6+3 = 80(9) 0 attn_shape = (2, 9)\n",
      "Epoch 10. Train Loss: 0.1079236, Test Loss : 0.07669519\n",
      "Epoch 11. Train Loss: 0.0872396, Test Loss : 0.07918824\n",
      "Epoch 12. Train Loss: 0.07187601, Test Loss : 0.073022656\n",
      "Epoch 13. Train Loss: 0.060432676, Test Loss : 0.05329684\n",
      "Epoch 14. Train Loss: 0.052742228, Test Loss : 0.046110235\n",
      "Epoch 15. Train Loss: 0.044229724, Test Loss : 0.072093144\n",
      "Epoch 16. Train Loss: 0.037415728, Test Loss : 0.052603103\n",
      "Epoch 17. Train Loss: 0.033719454, Test Loss : 0.032590374\n",
      "Epoch 18. Train Loss: 0.031813357, Test Loss : 0.15302786\n",
      "Epoch 19. Train Loss: 0.027175087, Test Loss : 0.0301067\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 82+8 = 90(90) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 6+722 = 728(728) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 29+493 = 522(522) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 2+7 = 98(9) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 68+944 = 1022(1012) 0 attn_shape = (4, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 45+9 = 54(54) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 20+447 = 467(467) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 2+49 = 51(51) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 31+81 = 102(112) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00193814  0.00940504 -0.09434737 -0.10598818 -0.04954937]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 18+336 = 354(354) 1 attn_shape = (3, 9)\n",
      "Epoch 20. Train Loss: 0.027013283, Test Loss : 0.023981526\n",
      "Epoch 21. Train Loss: 0.02381856, Test Loss : 0.017173339\n",
      "Epoch 22. Train Loss: 0.018520217, Test Loss : 0.022626068\n",
      "Epoch 23. Train Loss: 0.018722478, Test Loss : 0.032282356\n",
      "Epoch 24. Train Loss: 0.01753369, Test Loss : 0.02488554\n",
      "Epoch 25. Train Loss: 0.01557326, Test Loss : 0.024088617\n",
      "Epoch 26. Train Loss: 0.01473845, Test Loss : 0.012508261\n",
      "Epoch 27. Train Loss: 0.013114715, Test Loss : 0.014879353\n",
      "Epoch 28. Train Loss: 0.013857561, Test Loss : 0.043811556\n",
      "Epoch 29. Train Loss: 0.012906879, Test Loss : 0.012756963\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 654+0 = 654(654) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 93+0 = 94(93) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 2+92 = 31(94) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 625+3 = 628(628) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 76+459 = 535(535) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 112+651 = 763(763) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 485+431 = 916(916) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 52+832 = 884(884) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 52+9 = 61(61) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00616512  0.00848636 -0.08276179 -0.10928227 -0.04839264]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 793+380 = 1173(1173) 1 attn_shape = (4, 9)\n",
      "Epoch 30. Train Loss: 0.010428852, Test Loss : 0.01898906\n",
      "Epoch 31. Train Loss: 0.010462298, Test Loss : 0.008301606\n",
      "Epoch 32. Train Loss: 0.008919037, Test Loss : 0.014920225\n",
      "Epoch 33. Train Loss: 0.009775567, Test Loss : 0.012740584\n",
      "Epoch 34. Train Loss: 0.00833927, Test Loss : 0.025964279\n",
      "Epoch 35. Train Loss: 0.008226848, Test Loss : 0.015446641\n",
      "Epoch 36. Train Loss: 0.0069653024, Test Loss : 0.03423996\n",
      "Epoch 37. Train Loss: 0.009146839, Test Loss : 0.011556366\n",
      "Epoch 38. Train Loss: 0.0059430315, Test Loss : 0.030254468\n",
      "Epoch 39. Train Loss: 0.0067315917, Test Loss : 0.007950627\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 825+40 = 865(865) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 9+7 = 16(16) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 79+92 = 171(171) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 76+28 = 104(104) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 6+4 = 91(10) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 186+551 = 737(737) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 63+30 = 93(93) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 4+66 = 71(70) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 9+50 = 99(59) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.00620083 -0.00434285 -0.07164307 -0.11959682 -0.02257537]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 70+84 = 153(154) 0 attn_shape = (3, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: 0.006232758, Test Loss : 0.010733831\n",
      "Epoch 41. Train Loss: 0.006125392, Test Loss : 0.009932216\n",
      "Epoch 42. Train Loss: 0.005018495, Test Loss : 0.022587363\n",
      "Epoch 43. Train Loss: 0.006231102, Test Loss : 0.014154404\n",
      "Epoch 44. Train Loss: 0.006236558, Test Loss : 0.008611107\n",
      "Epoch 45. Train Loss: 0.0043496694, Test Loss : 0.010093014\n",
      "Epoch 46. Train Loss: 0.006096745, Test Loss : 0.00811981\n",
      "Epoch 47. Train Loss: 0.0046006893, Test Loss : 0.010398277\n",
      "Epoch 48. Train Loss: 0.00455107, Test Loss : 0.010639155\n",
      "Epoch 49. Train Loss: 0.004599923, Test Loss : 0.0073164045\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 9+5 = 14(14) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 65+52 = 107(117) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 3+810 = 813(813) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 159+368 = 527(527) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 64+9 = 73(73) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 81+37 = 118(118) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 80+57 = 137(137) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 17+71 = 88(88) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 0+563 = 563(563) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.01060041  0.00712864 -0.08961804 -0.11397053 -0.03082504]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 96+98 = 194(194) 1 attn_shape = (3, 9)\n",
      "Epoch 50. Train Loss: 0.0031991357, Test Loss : 0.011159575\n",
      "Epoch 51. Train Loss: 0.0034767957, Test Loss : 0.005192034\n",
      "Epoch 52. Train Loss: 0.0044140113, Test Loss : 0.020372253\n",
      "Epoch 53. Train Loss: 0.0026759894, Test Loss : 0.033021703\n",
      "Epoch 54. Train Loss: 0.003168826, Test Loss : 0.0072762854\n",
      "Epoch 55. Train Loss: 0.0035673408, Test Loss : 0.010234711\n",
      "Epoch 56. Train Loss: 0.0028329582, Test Loss : 0.006228308\n",
      "Epoch 57. Train Loss: 0.0039369394, Test Loss : 0.003996036\n",
      "Epoch 58. Train Loss: 0.0033124103, Test Loss : 0.0079223905\n",
      "Epoch 59. Train Loss: 0.0040240064, Test Loss : 0.010684685\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 444+13 = 457(457) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 2+89 = 101(91) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 46+336 = 382(382) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 94+89 = 184(183) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 680+48 = 728(728) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 2+122 = 234(124) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 0+56 = 66(56) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 425+103 = 528(528) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 797+2 = 809(799) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[-0.00481615  0.01091314 -0.09284496 -0.12132185 -0.03097919]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 6+3 = 90(9) 0 attn_shape = (2, 9)\n",
      "Epoch 60. Train Loss: 0.0028651408, Test Loss : 0.018296149\n",
      "Epoch 61. Train Loss: 0.0029496008, Test Loss : 0.00807544\n",
      "Epoch 62. Train Loss: 0.0028788738, Test Loss : 0.004972018\n",
      "Epoch 63. Train Loss: 0.0024427667, Test Loss : 0.005606456\n",
      "Epoch 64. Train Loss: 0.002439735, Test Loss : 0.021450037\n",
      "Epoch 65. Train Loss: 0.0029033555, Test Loss : 0.009036625\n",
      "Epoch 66. Train Loss: 0.0016924993, Test Loss : 0.0068911314\n",
      "Epoch 67. Train Loss: 0.0021775267, Test Loss : 0.0051461277\n",
      "Epoch 68. Train Loss: 0.002466022, Test Loss : 0.0066637746\n",
      "Epoch 69. Train Loss: 0.0016567319, Test Loss : 0.006711025\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 50+7 = 57(57) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 9+546 = 455(555) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 106+936 = 1042(1042) 1 attn_shape = (4, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 34+68 = 102(102) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 6+745 = 551(751) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 381+730 = 1111(1111) 1 attn_shape = (4, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 25+7 = 32(32) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 135+26 = 161(161) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 28+40 = 68(68) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00607066 -0.00085944 -0.0816493  -0.11492885 -0.03809799]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 52+6 = 58(58) 1 attn_shape = (2, 9)\n",
      "Epoch 70. Train Loss: 0.001809331, Test Loss : 0.007280505\n",
      "Epoch 71. Train Loss: 0.002200356, Test Loss : 0.009477181\n",
      "Epoch 72. Train Loss: 0.00347676, Test Loss : 0.045567002\n",
      "Epoch 73. Train Loss: 0.0017089858, Test Loss : 0.010615831\n",
      "Epoch 74. Train Loss: 0.0022435708, Test Loss : 0.008304689\n",
      "Epoch 75. Train Loss: 0.0021321406, Test Loss : 0.007439236\n",
      "Epoch 76. Train Loss: 0.0009611999, Test Loss : 0.00369464\n",
      "Epoch 77. Train Loss: 0.0003738343, Test Loss : 0.0051834816\n",
      "Epoch 78. Train Loss: 0.0014868036, Test Loss : 0.0057445937\n",
      "Epoch 79. Train Loss: 0.0012457009, Test Loss : 0.0059581394\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 6+8 = 142(14) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 0+3 = 4(3) 0 attn_shape = (1, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 23+704 = 727(727) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 4+632 = 636(636) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 64+5 = 79(69) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 9+92 = 101(101) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 536+241 = 777(777) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 5+79 = 94(84) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 6+68 = 84(74) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.0024336  -0.00088017 -0.08082228 -0.11051207 -0.02753314]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 9+713 = 822(722) 0 attn_shape = (3, 9)\n",
      "Epoch 80. Train Loss: 0.0017505941, Test Loss : 0.0087476075\n",
      "Epoch 81. Train Loss: 0.0029023977, Test Loss : 0.009425839\n",
      "Epoch 82. Train Loss: 0.000213686, Test Loss : 0.0036810054\n",
      "Epoch 83. Train Loss: 3.9202394e-05, Test Loss : 0.0029998224\n",
      "Epoch 84. Train Loss: 2.3498696e-05, Test Loss : 0.00291488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85. Train Loss: 2.0782567e-05, Test Loss : 0.002742657\n",
      "Epoch 86. Train Loss: 1.732663e-05, Test Loss : 0.0027863178\n",
      "Epoch 87. Train Loss: 1.642718e-05, Test Loss : 0.0026536891\n",
      "Epoch 88. Train Loss: 1.4983522e-05, Test Loss : 0.002749688\n",
      "Epoch 89. Train Loss: 1.4317726e-05, Test Loss : 0.0027421915\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 6+28 = 34(34) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 8+4 = 12(12) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 5+4 = 90(9) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 704+7 = 711(711) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 4+4 = 89(8) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 357+12 = 369(369) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 74+38 = 122(112) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 758+1 = 759(759) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 9+3 = 62(12) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00627833 -0.00127076 -0.0829793  -0.11610938 -0.02960343]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 1+139 = 240(140) 0 attn_shape = (3, 9)\n",
      "Epoch 90. Train Loss: 1.3287721e-05, Test Loss : 0.0026352867\n",
      "Epoch 91. Train Loss: 1.2301252e-05, Test Loss : 0.002587804\n",
      "Epoch 92. Train Loss: 1.1683474e-05, Test Loss : 0.0025955834\n",
      "Epoch 93. Train Loss: 1.1278176e-05, Test Loss : 0.0025005494\n",
      "Epoch 94. Train Loss: 1.0431846e-05, Test Loss : 0.0025620605\n",
      "Epoch 95. Train Loss: 1.0536015e-05, Test Loss : 0.0025804348\n",
      "Epoch 96. Train Loss: 1.000485e-05, Test Loss : 0.0024512894\n",
      "Epoch 97. Train Loss: 9.532847e-06, Test Loss : 0.0028177109\n",
      "Epoch 98. Train Loss: 9.325414e-06, Test Loss : 0.002561244\n",
      "Epoch 99. Train Loss: 8.795669e-06, Test Loss : 0.0025373562\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 4+77 = 92(81) 0 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 9+5 = 14(14) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 61+329 = 390(390) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 35+81 = 106(116) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 84+6 = 90(90) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[91m☒\u001b[0m 0+542 = 442(542) 0 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 8+4 = 12(12) 1 attn_shape = (2, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 55+702 = 757(757) 1 attn_shape = (3, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 59+980 = 1039(1039) 1 attn_shape = (4, 9)\n",
      "cov = \n",
      "[ 0.00632387 -0.00122371 -0.08303399 -0.11632677 -0.02962764]\n",
      "<NDArray 5 @gpu(0)>\n",
      "\u001b[92m☑\u001b[0m 9+402 = 411(411) 1 attn_shape = (3, 9)\n",
      "Epoch 100. Train Loss: 8.741921e-06, Test Loss : 0.0025269678\n"
     ]
    }
   ],
   "source": [
    "epochs = 101\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p, attn = model.calculate(q[i], char_indices, indices_char)\n",
    "                print('cov = {}'.format(model.alignment.weight.data()[0, :5]))\n",
    "                p = p.strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) {} attn_shape = {}\".format(q[i], p, y[i], str(iscorr), attn.shape))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(net, data, ctx = mx.cpu()):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    p =[]\n",
    "    attn = []\n",
    "    for i, d  in enumerate(data):\n",
    "        _p, _attn = net.calculate(d, char_indices, indices_char, input_digits = 9, lchars = len(chars))\n",
    "        p.append(_p.strip())\n",
    "        attn.append(_attn)\n",
    "\n",
    "    fig, axes = plt.subplots(np.int(np.ceil(len(data) / 1)), 1, sharex = False, sharey = False)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "\n",
    "    if len(data) > 1:\n",
    "        fig.set_size_inches(5, 40)\n",
    "    else:\n",
    "        fig.set_size_inches(10, 10)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    \n",
    "    for i, (d, p, a) in enumerate(zip(data, p, attn)):\n",
    "        _col = list(d)\n",
    "        _idx = list(p)\n",
    "        print(a.shape)\n",
    "        _val = a[:len(p), :len(d)]\n",
    "        \n",
    "        print('input: {}, length: {}'.format(d,len(d)))\n",
    "        print('calculated: {}, length:{}'.format(p,len(p)))\n",
    "        print('attention shape= {}'.format(a.shape))\n",
    "        print('check attn = {}'.format(np.sum(a, axis = 1)))\n",
    "        print('val shape= {}'.format(_val.shape))\n",
    "        if len(data) > 1:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3)\n",
    "        else:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), cmap = 'RdYlGn', linewidths = .3)\n",
    "        #axes.flat[i].set_title('Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "input: 514+781, length: 7\n",
      "calculated: 1295, length:4\n",
      "attention shape= (4, 9)\n",
      "check attn = [1.        1.        1.        0.9999999]\n",
      "val shape= (4, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gen_eqn():\n",
    "    a, b = n(), n()      \n",
    "    return '{}+{}'.format(a, b)\n",
    "\n",
    "example = [gen_eqn() for _ in range(1)]\n",
    "plot_attention(model, example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
