{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sys.path.append(\"../python/\")\n",
    "from date_format_translator_additive_attention import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "in_seq_len = 32\n",
    "out_seq_len = 32\n",
    "\n",
    "X, Y, Z, chars, char_indices, indices_char = generate_date_data(N)\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)\n",
    "\n",
    "### Testset\n",
    "\n",
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, loss, and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = format_translator(300, in_seq_len, out_seq_len, len(chars), ctx)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 2002-September-25 Wed = 0(09/25/2002, wednesday) 0\n",
      "\u001b[91m☒\u001b[0m 2012-January-28 Sat = 0(01/28/2012, saturday) 0\n",
      "\u001b[91m☒\u001b[0m 2008-October-25 Sat = 0(10/25/2008, saturday) 0\n",
      "\u001b[91m☒\u001b[0m 2011-October-28 Fri = 0(10/28/2011, friday) 0\n",
      "\u001b[91m☒\u001b[0m 2002-November-16 Sat = 0(11/16/2002, saturday) 0\n",
      "\u001b[91m☒\u001b[0m 2003-January-05 Sun = 0(01/05/2003, sunday) 0\n",
      "\u001b[91m☒\u001b[0m 2006-July-17 Mon = 0(07/17/2006, monday) 0\n",
      "\u001b[91m☒\u001b[0m 2005-September-18 Sun = 0(09/18/2005, sunday) 0\n",
      "\u001b[91m☒\u001b[0m 2005-January-28 Fri = 0(01/28/2005, friday) 0\n",
      "\u001b[91m☒\u001b[0m 2007-May-22 Tue = 0(05/22/2007, tuesday) 0\n",
      "Epoch 0. Train Loss: 2.296412, Test Loss : 3.677928\n",
      "Epoch 1. Train Loss: 0.77523667, Test Loss : 3.6236515\n",
      "Epoch 2. Train Loss: 0.559, Test Loss : 3.5865119\n",
      "Epoch 3. Train Loss: 0.42468655, Test Loss : 3.5381057\n",
      "Epoch 4. Train Loss: 0.38893643, Test Loss : 3.5392697\n",
      "Epoch 5. Train Loss: 0.3250221, Test Loss : 3.4319785\n",
      "Epoch 6. Train Loss: 0.25021434, Test Loss : 3.3121357\n",
      "Epoch 7. Train Loss: 0.22498292, Test Loss : 3.1656008\n",
      "Epoch 8. Train Loss: 0.14613499, Test Loss : 3.100744\n",
      "Epoch 9. Train Loss: 0.1746178, Test Loss : 2.8577783\n",
      "\u001b[91m☒\u001b[0m 2002-June-11 Tue = 06/11/202/202, tuesday(06/11/2002, tuesday) 0\n",
      "\u001b[91m☒\u001b[0m 2003-May-18 Sun = 05/1/20/202, sunday(05/18/2003, sunday) 0\n",
      "\u001b[91m☒\u001b[0m 2001-December-05 Wed = 1/2/05/2012, wednesday(12/05/2001, wednesday) 0\n",
      "\u001b[91m☒\u001b[0m 2003-November-16 Sun = 11/16/2006, sunday(11/16/2003, sunday) 0\n",
      "\u001b[91m☒\u001b[0m 2005-February-10 Thu = 0/1/20/2005, thursday(02/10/2005, thursday) 0\n",
      "\u001b[91m☒\u001b[0m 2008-December-26 Fri = 12/26/22008, friday(12/26/2008, friday) 0\n",
      "\u001b[91m☒\u001b[0m 2003-November-16 Sun = 11/16/2006, sunday(11/16/2003, sunday) 0\n",
      "\u001b[91m☒\u001b[0m 2006-March-25 Sat = 03/25/202/2006, saturday(03/25/2006, saturday) 0\n",
      "\u001b[91m☒\u001b[0m 2010-April-20 Tue = 04/2/201/202, tuesday(04/20/2010, tuesday) 0\n",
      "\u001b[91m☒\u001b[0m 2008-September-13 Sat = 09/1/12/0208, saturday(09/13/2008, saturday) 0\n",
      "Epoch 10. Train Loss: 0.09160434, Test Loss : 2.632884\n",
      "Epoch 11. Train Loss: 0.106204286, Test Loss : 2.3677163\n",
      "Epoch 12. Train Loss: 0.044445816, Test Loss : 1.9968698\n",
      "Epoch 13. Train Loss: 0.033939254, Test Loss : 1.5771114\n",
      "Epoch 14. Train Loss: 0.035526708, Test Loss : 1.1317021\n",
      "Epoch 15. Train Loss: 0.084716395, Test Loss : 0.78327036\n",
      "Epoch 16. Train Loss: 0.046230204, Test Loss : 0.5313473\n",
      "Epoch 17. Train Loss: 0.016118571, Test Loss : 0.28550717\n",
      "Epoch 18. Train Loss: 0.012300772, Test Loss : 0.14958413\n",
      "Epoch 19. Train Loss: 0.011537328, Test Loss : 0.087371185\n",
      "\u001b[92m☑\u001b[0m 2003-January-28 Tue = 01/28/2003, tuesday(01/28/2003, tuesday) 1\n",
      "\u001b[91m☒\u001b[0m 2010-July-02 Fri = 07/20/2010, friday(07/02/2010, friday) 0\n",
      "\u001b[91m☒\u001b[0m 2007-January-02 Tue = 01/20/2007, tuesday(01/02/2007, tuesday) 0\n",
      "\u001b[92m☑\u001b[0m 2007-January-27 Sat = 01/27/2007, saturday(01/27/2007, saturday) 1\n",
      "\u001b[91m☒\u001b[0m 2010-August-12 Thu = 08/21/2010, thursday(08/12/2010, thursday) 0\n",
      "\u001b[92m☑\u001b[0m 2008-April-20 Sun = 04/20/2008, sunday(04/20/2008, sunday) 1\n",
      "\u001b[91m☒\u001b[0m 2008-June-05 Thu = 06/20/2008, thursday(06/05/2008, thursday) 0\n",
      "\u001b[92m☑\u001b[0m 2010-February-17 Wed = 02/17/2010, wednesday(02/17/2010, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-August-16 Tue = 08/16/2005, tuesday(08/16/2005, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-March-29 Sat = 03/29/2008, saturday(03/29/2008, saturday) 1\n",
      "Epoch 20. Train Loss: 0.03459299, Test Loss : 0.08303594\n",
      "Epoch 21. Train Loss: 0.022106187, Test Loss : 0.055378422\n",
      "Epoch 22. Train Loss: 0.009654053, Test Loss : 0.017762613\n",
      "Epoch 23. Train Loss: 0.0051350994, Test Loss : 0.01046652\n",
      "Epoch 24. Train Loss: 0.004417262, Test Loss : 0.007783762\n",
      "Epoch 25. Train Loss: 0.0038695172, Test Loss : 0.005786612\n",
      "Epoch 26. Train Loss: 0.0032227987, Test Loss : 0.0046019694\n",
      "Epoch 27. Train Loss: 0.0027913696, Test Loss : 0.0039937473\n",
      "Epoch 28. Train Loss: 0.0024555125, Test Loss : 0.0029155572\n",
      "Epoch 29. Train Loss: 0.003813596, Test Loss : 0.03356776\n",
      "\u001b[91m☒\u001b[0m 2009-June-16 Tue = 06/16/2002, tuesday(06/16/2009, tuesday) 0\n",
      "\u001b[91m☒\u001b[0m 2002-April-30 Tue = 04/20/2002, tuesday(04/30/2002, tuesday) 0\n",
      "\u001b[91m☒\u001b[0m 2004-October-15 Fri = 10/15/2002, friday(10/15/2004, friday) 0\n",
      "\u001b[91m☒\u001b[0m 2006-February-13 Mon = 02/13/2002, monday(02/13/2006, monday) 0\n",
      "\u001b[91m☒\u001b[0m 2008-November-24 Mon = 11/24/2002, monday(11/24/2008, monday) 0\n",
      "\u001b[91m☒\u001b[0m 2005-December-24 Sat = 12/24/2002, saturday(12/24/2005, saturday) 0\n",
      "\u001b[91m☒\u001b[0m 2005-October-14 Fri = 10/14/2002, friday(10/14/2005, friday) 0\n",
      "\u001b[91m☒\u001b[0m 2003-June-20 Fri = 06/20/2002, friday(06/20/2003, friday) 0\n",
      "\u001b[92m☑\u001b[0m 2002-September-26 Thu = 09/26/2002, thursday(09/26/2002, thursday) 1\n",
      "\u001b[91m☒\u001b[0m 2007-June-28 Thu = 06/28/2002, thursday(06/28/2007, thursday) 0\n",
      "Epoch 30. Train Loss: 0.19558531, Test Loss : 0.17767563\n",
      "Epoch 31. Train Loss: 0.06115856, Test Loss : 0.017290106\n",
      "Epoch 32. Train Loss: 0.006071995, Test Loss : 0.006680246\n",
      "Epoch 33. Train Loss: 0.0038080993, Test Loss : 0.004272043\n",
      "Epoch 34. Train Loss: 0.0029291168, Test Loss : 0.0034743028\n",
      "Epoch 35. Train Loss: 0.0025765141, Test Loss : 0.0026900482\n",
      "Epoch 36. Train Loss: 0.002129478, Test Loss : 0.002402902\n",
      "Epoch 37. Train Loss: 0.0019381198, Test Loss : 0.002207408\n",
      "Epoch 38. Train Loss: 0.0017031834, Test Loss : 0.0018011066\n",
      "Epoch 39. Train Loss: 0.0014921168, Test Loss : 0.0017005223\n",
      "\u001b[92m☑\u001b[0m 2004-August-22 Sun = 08/22/2004, sunday(08/22/2004, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-March-26 Sat = 03/26/2005, saturday(03/26/2005, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-August-27 Sun = 08/27/2006, sunday(08/27/2006, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-January-24 Sat = 01/24/2009, saturday(01/24/2009, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-December-30 Sun = 12/30/2007, sunday(12/30/2007, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-March-03 Thu = 03/03/2011, thursday(03/03/2011, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-March-11 Sun = 03/11/2007, sunday(03/11/2007, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-May-16 Sun = 05/16/2010, sunday(05/16/2010, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-January-14 Mon = 01/14/2002, monday(01/14/2002, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-December-30 Sun = 12/30/2007, sunday(12/30/2007, sunday) 1\n",
      "Epoch 40. Train Loss: 0.0013647084, Test Loss : 0.0014306678\n",
      "Epoch 41. Train Loss: 0.0012230779, Test Loss : 0.0013683811\n",
      "Epoch 42. Train Loss: 0.0010692925, Test Loss : 0.001212874\n",
      "Epoch 43. Train Loss: 0.0009823359, Test Loss : 0.0011752263\n",
      "Epoch 44. Train Loss: 0.0008834711, Test Loss : 0.00092477433\n",
      "Epoch 45. Train Loss: 0.0007850937, Test Loss : 0.0008775624\n",
      "Epoch 46. Train Loss: 0.00071037555, Test Loss : 0.00080670416\n",
      "Epoch 47. Train Loss: 0.00065356435, Test Loss : 0.00074405916\n",
      "Epoch 48. Train Loss: 0.0006039959, Test Loss : 0.0007215393\n",
      "Epoch 49. Train Loss: 0.0005370943, Test Loss : 0.00059582724\n",
      "\u001b[92m☑\u001b[0m 2008-September-30 Tue = 09/30/2008, tuesday(09/30/2008, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2012-June-10 Sun = 06/10/2012, sunday(06/10/2012, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-March-14 Mon = 03/14/2011, monday(03/14/2011, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-December-11 Sat = 12/11/2010, saturday(12/11/2010, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-February-07 Mon = 02/07/2005, monday(02/07/2005, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-November-09 Fri = 11/09/2007, friday(11/09/2007, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-July-31 Sun = 07/31/2005, sunday(07/31/2005, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-January-03 Sat = 01/03/2004, saturday(01/03/2004, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-March-17 Wed = 03/17/2010, wednesday(03/17/2010, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-August-26 Sun = 08/26/2007, sunday(08/26/2007, sunday) 1\n",
      "Epoch 50. Train Loss: 0.00048648677, Test Loss : 0.00055442797\n",
      "Epoch 51. Train Loss: 0.00044467216, Test Loss : 0.0006051641\n",
      "Epoch 52. Train Loss: 0.00040793204, Test Loss : 0.0004603226\n",
      "Epoch 53. Train Loss: 0.00038093427, Test Loss : 0.00048195702\n",
      "Epoch 54. Train Loss: 0.00034742698, Test Loss : 0.00040864624\n",
      "Epoch 55. Train Loss: 0.00031762573, Test Loss : 0.00037584346\n",
      "Epoch 56. Train Loss: 0.00029882014, Test Loss : 0.00034851994\n",
      "Epoch 57. Train Loss: 0.0002902936, Test Loss : 0.00040250755\n",
      "Epoch 58. Train Loss: 0.00030261354, Test Loss : 0.00067476364\n",
      "Epoch 59. Train Loss: 0.112832665, Test Loss : 0.1653676\n",
      "\u001b[92m☑\u001b[0m 2006-January-28 Sat = 01/28/2006, saturday(01/28/2006, saturday) 1\n",
      "\u001b[91m☒\u001b[0m 2010-March-31 Wed = 03/31/2001, wednesday(03/31/2010, wednesday) 0\n",
      "\u001b[91m☒\u001b[0m 2010-February-19 Fri = 02/19/2001, friday(02/19/2010, friday) 0\n",
      "\u001b[92m☑\u001b[0m 2003-March-31 Mon = 03/31/2003, monday(03/31/2003, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2003-August-11 Mon = 08/11/2003, monday(08/11/2003, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-November-13 Mon = 11/13/2006, monday(11/13/2006, monday) 1\n",
      "\u001b[91m☒\u001b[0m 2005-February-21 Mon = 02/210/2005, monday(02/21/2005, monday) 0\n",
      "\u001b[91m☒\u001b[0m 2005-October-28 Fri = 01/28/2005, friday(10/28/2005, friday) 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m☑\u001b[0m 2008-December-06 Sat = 12/06/2008, saturday(12/06/2008, saturday) 1\n",
      "\u001b[91m☒\u001b[0m 2005-February-12 Sat = 02/12/005, saturday(02/12/2005, saturday) 0\n",
      "Epoch 60. Train Loss: 0.039960306, Test Loss : 0.03351063\n",
      "Epoch 61. Train Loss: 0.0014085108, Test Loss : 0.008091836\n",
      "Epoch 62. Train Loss: 0.0010529436, Test Loss : 0.0026865327\n",
      "Epoch 63. Train Loss: 0.00081141066, Test Loss : 0.001277386\n",
      "Epoch 64. Train Loss: 0.00070599065, Test Loss : 0.000920999\n",
      "Epoch 65. Train Loss: 0.0006102984, Test Loss : 0.0007822863\n",
      "Epoch 66. Train Loss: 0.00054282945, Test Loss : 0.0007039012\n",
      "Epoch 67. Train Loss: 0.0005038419, Test Loss : 0.0006289857\n",
      "Epoch 68. Train Loss: 0.00046359826, Test Loss : 0.00058030494\n",
      "Epoch 69. Train Loss: 0.00042181712, Test Loss : 0.0004936303\n",
      "\u001b[92m☑\u001b[0m 2002-October-07 Mon = 10/07/2002, monday(10/07/2002, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2003-December-19 Fri = 12/19/2003, friday(12/19/2003, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-January-21 Fri = 01/21/2005, friday(01/21/2005, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-December-17 Tue = 12/17/2002, tuesday(12/17/2002, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2003-April-25 Fri = 04/25/2003, friday(04/25/2003, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-October-19 Thu = 10/19/2006, thursday(10/19/2006, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-June-15 Wed = 06/15/2011, wednesday(06/15/2011, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-June-10 Fri = 06/10/2005, friday(06/10/2005, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-July-05 Thu = 07/05/2007, thursday(07/05/2007, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-November-25 Sun = 11/25/2007, sunday(11/25/2007, sunday) 1\n",
      "Epoch 70. Train Loss: 0.00038639584, Test Loss : 0.000495278\n",
      "Epoch 71. Train Loss: 0.0003543876, Test Loss : 0.0004487119\n",
      "Epoch 72. Train Loss: 0.00033427856, Test Loss : 0.00042504017\n",
      "Epoch 73. Train Loss: 0.00030832365, Test Loss : 0.00041646487\n",
      "Epoch 74. Train Loss: 0.00029045783, Test Loss : 0.0003455539\n",
      "Epoch 75. Train Loss: 0.00027076548, Test Loss : 0.0003549143\n",
      "Epoch 76. Train Loss: 0.0002534342, Test Loss : 0.00029922067\n",
      "Epoch 77. Train Loss: 0.00024119145, Test Loss : 0.00029726874\n",
      "Epoch 78. Train Loss: 0.00022069122, Test Loss : 0.00029531494\n",
      "Epoch 79. Train Loss: 0.000206951, Test Loss : 0.0002650877\n",
      "\u001b[92m☑\u001b[0m 2011-December-15 Thu = 12/15/2011, thursday(12/15/2011, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-September-07 Sat = 09/07/2002, saturday(09/07/2002, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-November-08 Sat = 11/08/2008, saturday(11/08/2008, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-July-09 Tue = 07/09/2002, tuesday(07/09/2002, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-April-19 Sun = 04/19/2009, sunday(04/19/2009, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-October-29 Sat = 10/29/2005, saturday(10/29/2005, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-May-23 Sun = 05/23/2010, sunday(05/23/2010, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-May-05 Sun = 05/05/2002, sunday(05/05/2002, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-April-03 Sun = 04/03/2005, sunday(04/03/2005, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-February-29 Sun = 02/29/2004, sunday(02/29/2004, sunday) 1\n",
      "Epoch 80. Train Loss: 0.00019403879, Test Loss : 0.0002725492\n",
      "Epoch 81. Train Loss: 0.00018128533, Test Loss : 0.00026235922\n",
      "Epoch 82. Train Loss: 0.00017227198, Test Loss : 0.00020654695\n",
      "Epoch 83. Train Loss: 0.00016286774, Test Loss : 0.00021846552\n",
      "Epoch 84. Train Loss: 0.00015519872, Test Loss : 0.00019038258\n",
      "Epoch 85. Train Loss: 0.0001479983, Test Loss : 0.00022432068\n",
      "Epoch 86. Train Loss: 0.00014164115, Test Loss : 0.00023102268\n",
      "Epoch 87. Train Loss: 0.00013637704, Test Loss : 0.00018513683\n",
      "Epoch 88. Train Loss: 0.00012586024, Test Loss : 0.00017166595\n",
      "Epoch 89. Train Loss: 0.00012174128, Test Loss : 0.00014767009\n",
      "\u001b[92m☑\u001b[0m 2012-May-28 Mon = 05/28/2012, monday(05/28/2012, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-August-28 Fri = 08/28/2009, friday(08/28/2009, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-August-02 Sun = 08/02/2009, sunday(08/02/2009, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-October-17 Sun = 10/17/2010, sunday(10/17/2010, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-September-30 Fri = 09/30/2011, friday(09/30/2011, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-October-30 Tue = 10/30/2007, tuesday(10/30/2007, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-September-06 Mon = 09/06/2004, monday(09/06/2004, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-November-17 Thu = 11/17/2005, thursday(11/17/2005, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-October-05 Sat = 10/05/2002, saturday(10/05/2002, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2012-July-28 Sat = 07/28/2012, saturday(07/28/2012, saturday) 1\n",
      "Epoch 90. Train Loss: 0.000120751894, Test Loss : 0.00017503966\n",
      "Epoch 91. Train Loss: 0.00012245914, Test Loss : 0.00026293762\n",
      "Epoch 92. Train Loss: 0.00011214205, Test Loss : 0.00014875461\n",
      "Epoch 93. Train Loss: 0.00010304294, Test Loss : 0.00012983526\n",
      "Epoch 94. Train Loss: 0.00010114164, Test Loss : 0.00013243385\n",
      "Epoch 95. Train Loss: 9.721415e-05, Test Loss : 0.00013807105\n",
      "Epoch 96. Train Loss: 9.523652e-05, Test Loss : 0.000118841206\n",
      "Epoch 97. Train Loss: 9.172247e-05, Test Loss : 0.00013175803\n",
      "Epoch 98. Train Loss: 8.955042e-05, Test Loss : 0.000121730336\n",
      "Epoch 99. Train Loss: 8.731513e-05, Test Loss : 0.00012177734\n",
      "\u001b[92m☑\u001b[0m 2008-December-31 Wed = 12/31/2008, wednesday(12/31/2008, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-February-22 Sun = 02/22/2004, sunday(02/22/2004, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-December-18 Thu = 12/18/2008, thursday(12/18/2008, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-February-27 Fri = 02/27/2009, friday(02/27/2009, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-April-07 Mon = 04/07/2008, monday(04/07/2008, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-April-08 Sun = 04/08/2007, sunday(04/08/2007, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-December-07 Tue = 12/07/2010, tuesday(12/07/2010, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-October-27 Mon = 10/27/2008, monday(10/27/2008, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-September-14 Fri = 09/14/2007, friday(09/14/2007, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-September-04 Wed = 09/04/2002, wednesday(09/04/2002, wednesday) 1\n",
      "Epoch 100. Train Loss: 8.3428924e-05, Test Loss : 0.00010847732\n",
      "Epoch 101. Train Loss: 8.163487e-05, Test Loss : 0.00011231992\n",
      "Epoch 102. Train Loss: 7.962156e-05, Test Loss : 0.00010219329\n",
      "Epoch 103. Train Loss: 7.74591e-05, Test Loss : 0.0001020217\n",
      "Epoch 104. Train Loss: 7.482523e-05, Test Loss : 9.198717e-05\n",
      "Epoch 105. Train Loss: 7.4682444e-05, Test Loss : 9.47401e-05\n",
      "Epoch 106. Train Loss: 7.252653e-05, Test Loss : 9.154479e-05\n",
      "Epoch 107. Train Loss: 6.985553e-05, Test Loss : 8.7906985e-05\n",
      "Epoch 108. Train Loss: 6.9211004e-05, Test Loss : 9.081778e-05\n",
      "Epoch 109. Train Loss: 6.8020934e-05, Test Loss : 8.2254555e-05\n",
      "\u001b[92m☑\u001b[0m 2012-January-26 Thu = 01/26/2012, thursday(01/26/2012, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-September-13 Tue = 09/13/2011, tuesday(09/13/2011, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-August-04 Wed = 08/04/2004, wednesday(08/04/2004, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-June-08 Fri = 06/08/2007, friday(06/08/2007, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-August-05 Thu = 08/05/2004, thursday(08/05/2004, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-October-30 Sat = 10/30/2004, saturday(10/30/2004, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-October-01 Mon = 10/01/2007, monday(10/01/2007, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-October-31 Sun = 10/31/2010, sunday(10/31/2010, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-December-14 Tue = 12/14/2004, tuesday(12/14/2004, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2012-February-22 Wed = 02/22/2012, wednesday(02/22/2012, wednesday) 1\n",
      "Epoch 110. Train Loss: 6.56078e-05, Test Loss : 8.117277e-05\n",
      "Epoch 111. Train Loss: 6.463614e-05, Test Loss : 7.962537e-05\n",
      "Epoch 112. Train Loss: 6.351407e-05, Test Loss : 7.816931e-05\n",
      "Epoch 113. Train Loss: 6.305151e-05, Test Loss : 7.462071e-05\n",
      "Epoch 114. Train Loss: 6.1185514e-05, Test Loss : 7.499911e-05\n",
      "Epoch 115. Train Loss: 6.132998e-05, Test Loss : 7.5190845e-05\n",
      "Epoch 116. Train Loss: 6.018898e-05, Test Loss : 7.08089e-05\n",
      "Epoch 117. Train Loss: 5.753278e-05, Test Loss : 7.0545844e-05\n",
      "Epoch 118. Train Loss: 5.7436253e-05, Test Loss : 7.1786264e-05\n",
      "Epoch 119. Train Loss: 5.688607e-05, Test Loss : 7.2169896e-05\n",
      "\u001b[92m☑\u001b[0m 2005-December-30 Fri = 12/30/2005, friday(12/30/2005, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2005-January-03 Mon = 01/03/2005, monday(01/03/2005, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-July-21 Tue = 07/21/2009, tuesday(07/21/2009, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-April-22 Wed = 04/22/2009, wednesday(04/22/2009, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-October-14 Thu = 10/14/2004, thursday(10/14/2004, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-October-29 Wed = 10/29/2008, wednesday(10/29/2008, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-November-23 Sun = 11/23/2008, sunday(11/23/2008, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-August-13 Mon = 08/13/2007, monday(08/13/2007, monday) 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m☑\u001b[0m 2009-April-03 Fri = 04/03/2009, friday(04/03/2009, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2004-March-14 Sun = 03/14/2004, sunday(03/14/2004, sunday) 1\n",
      "Epoch 120. Train Loss: 5.542883e-05, Test Loss : 6.939716e-05\n",
      "Epoch 121. Train Loss: 5.497368e-05, Test Loss : 6.713189e-05\n",
      "Epoch 122. Train Loss: 5.373539e-05, Test Loss : 6.637551e-05\n",
      "Epoch 123. Train Loss: 5.2451644e-05, Test Loss : 6.4224325e-05\n",
      "Epoch 124. Train Loss: 5.314114e-05, Test Loss : 6.4221604e-05\n",
      "Epoch 125. Train Loss: 5.1359617e-05, Test Loss : 6.106635e-05\n",
      "Epoch 126. Train Loss: 5.0736264e-05, Test Loss : 6.03837e-05\n",
      "Epoch 127. Train Loss: 4.97414e-05, Test Loss : 6.264934e-05\n",
      "Epoch 128. Train Loss: 4.931876e-05, Test Loss : 5.910715e-05\n",
      "Epoch 129. Train Loss: 4.8803562e-05, Test Loss : 5.819958e-05\n",
      "\u001b[92m☑\u001b[0m 2006-July-01 Sat = 07/01/2006, saturday(07/01/2006, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-August-22 Tue = 08/22/2006, tuesday(08/22/2006, tuesday) 1\n",
      "\u001b[92m☑\u001b[0m 2012-April-28 Sat = 04/28/2012, saturday(04/28/2012, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-June-21 Wed = 06/21/2006, wednesday(06/21/2006, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2002-May-31 Fri = 05/31/2002, friday(05/31/2002, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2003-May-15 Thu = 05/15/2003, thursday(05/15/2003, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-June-12 Sat = 06/12/2010, saturday(06/12/2010, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2012-May-26 Sat = 05/26/2012, saturday(05/26/2012, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-November-21 Mon = 11/21/2011, monday(11/21/2011, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-February-24 Fri = 02/24/2006, friday(02/24/2006, friday) 1\n",
      "Epoch 130. Train Loss: 4.7423444e-05, Test Loss : 5.8019123e-05\n",
      "Epoch 131. Train Loss: 4.679054e-05, Test Loss : 5.7787198e-05\n",
      "Epoch 132. Train Loss: 4.626187e-05, Test Loss : 5.5993165e-05\n",
      "Epoch 133. Train Loss: 4.5479115e-05, Test Loss : 5.522328e-05\n",
      "Epoch 134. Train Loss: 4.481731e-05, Test Loss : 5.512926e-05\n",
      "Epoch 135. Train Loss: 4.4958302e-05, Test Loss : 5.3420415e-05\n",
      "Epoch 136. Train Loss: 4.332264e-05, Test Loss : 5.352281e-05\n",
      "Epoch 137. Train Loss: 4.368228e-05, Test Loss : 5.2154486e-05\n",
      "Epoch 138. Train Loss: 4.2730408e-05, Test Loss : 5.2261847e-05\n",
      "Epoch 139. Train Loss: 4.2219304e-05, Test Loss : 5.205094e-05\n",
      "\u001b[92m☑\u001b[0m 2008-December-15 Mon = 12/15/2008, monday(12/15/2008, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2008-July-28 Mon = 07/28/2008, monday(07/28/2008, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2007-October-14 Sun = 10/14/2007, sunday(10/14/2007, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-May-20 Thu = 05/20/2010, thursday(05/20/2010, thursday) 1\n",
      "\u001b[92m☑\u001b[0m 2009-September-30 Wed = 09/30/2009, wednesday(09/30/2009, wednesday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-October-02 Sat = 10/02/2010, saturday(10/02/2010, saturday) 1\n",
      "\u001b[92m☑\u001b[0m 2011-April-25 Mon = 04/25/2011, monday(04/25/2011, monday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-April-23 Fri = 04/23/2010, friday(04/23/2010, friday) 1\n",
      "\u001b[92m☑\u001b[0m 2006-March-12 Sun = 03/12/2006, sunday(03/12/2006, sunday) 1\n",
      "\u001b[92m☑\u001b[0m 2010-February-28 Sun = 02/28/2010, sunday(02/28/2010, sunday) 1\n",
      "Epoch 140. Train Loss: 4.1916897e-05, Test Loss : 5.054821e-05\n",
      "Epoch 141. Train Loss: 4.0988198e-05, Test Loss : 5.0237522e-05\n",
      "Epoch 142. Train Loss: 4.0395866e-05, Test Loss : 5.0130402e-05\n",
      "Epoch 143. Train Loss: 4.049095e-05, Test Loss : 4.9576734e-05\n",
      "Epoch 144. Train Loss: 4.010468e-05, Test Loss : 4.8448288e-05\n"
     ]
    }
   ],
   "source": [
    "res = train(model, tr_data_iterator, te_data_iterator \\\n",
    "          , trainer, loss, char_indices, indices_char\n",
    "          , epochs= 300, ctx = ctx, output_file_name = 'additive_attention_{}'.format(datetime.now().strftime(%Y%m%d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2007-December-01 Sat, length: 20\n",
      "prediction: 12/01/2007, saturday, length:20\n",
      "attention shape= (20, 32)\n",
      "check attn = [0.9999999  0.99999994 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.0000001  1.\n",
      " 1.         1.         1.         1.         0.99999994 1.\n",
      " 1.         1.        ]\n",
      "val shape= (20, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = [gen_date() for _ in range(1)]\n",
    "plot_attention(model, example, char_indices, indices_char, in_seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
