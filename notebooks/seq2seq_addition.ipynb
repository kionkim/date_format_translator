{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        # Since we don't use intermediate states for 'though vector', we don't need to unroll it.\n",
    "        # In the later examples, we will use LSTM class rather than LSTMCell class.\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs\n",
    "                                                    , length = self.in_seq_len\n",
    "                                                    , merge_outputs = True)\n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(outputs[:, i, :], [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)\n",
    "            #print('i= {}, deouts= {}'.format(i, deouts.shape))\n",
    "        \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculation(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(deout, [next_h, next_c])\n",
    "            #print('dim deout = {}'.format(deout.shape))\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            #print('dim deout = {}'.format(deout.shape))\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            #print('dim deout = {}'.format(deout.shape))\n",
    "\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "        return ret_seq.strip('E').strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)\n",
    "\n",
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTMCell(None -> 1200)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 93+0 = 1000(93) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+43 = 449(43) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 864+752 = 1009(1616) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 175+82 = 1009(257) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 761+8 = 1039(769) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 90+421 = 1009(511) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 446+788 = 1039(1234) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 62+36 = 630(98) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 4+4 = 444(8) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 617+6 = 779(623) 1/0 0\n",
      "Epoch 0. Train Loss: 1.1954819, Test Loss : 1.1388888\n",
      "Epoch 1. Train Loss: 1.1225086, Test Loss : 1.1047744\n",
      "Epoch 2. Train Loss: 1.0937438, Test Loss : 1.0658474\n",
      "Epoch 3. Train Loss: 1.0264179, Test Loss : 0.97517955\n",
      "Epoch 4. Train Loss: 0.9184651, Test Loss : 0.86142606\n",
      "Epoch 5. Train Loss: 0.82754004, Test Loss : 0.7985453\n",
      "Epoch 6. Train Loss: 0.7551264, Test Loss : 0.77629375\n",
      "Epoch 7. Train Loss: 0.6870809, Test Loss : 0.66588545\n",
      "Epoch 8. Train Loss: 0.6154428, Test Loss : 0.6293352\n",
      "Epoch 9. Train Loss: 0.5184007, Test Loss : 0.4856955\n",
      "\u001b[91m☒\u001b[0m 53+905 = 968(958) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 3+7 = 141(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 50+7 = 57(57) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+66 = 70(69) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 73+166 = 239(239) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 111+472 = 563(583) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 0+821 = 821(821) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 829+471 = 1280(1300) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 641+2 = 644(643) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 64+1 = 77(65) 1/0 0\n",
      "Epoch 10. Train Loss: 0.39728454, Test Loss : 0.3630874\n",
      "Epoch 11. Train Loss: 0.28976592, Test Loss : 0.2227037\n",
      "Epoch 12. Train Loss: 0.2122907, Test Loss : 0.23804882\n",
      "Epoch 13. Train Loss: 0.1616686, Test Loss : 0.1725686\n",
      "Epoch 14. Train Loss: 0.124784224, Test Loss : 0.13457607\n",
      "Epoch 15. Train Loss: 0.10108661, Test Loss : 0.08819932\n",
      "Epoch 16. Train Loss: 0.08172799, Test Loss : 0.06092932\n",
      "Epoch 17. Train Loss: 0.07011767, Test Loss : 0.09531333\n",
      "Epoch 18. Train Loss: 0.060373805, Test Loss : 0.1443614\n",
      "Epoch 19. Train Loss: 0.053134788, Test Loss : 0.06308187\n",
      "\u001b[92m☑\u001b[0m 8+998 = 1006(1006) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 502+0 = 502(502) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 92+21 = 112(113) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 9+63 = 72(72) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 11+155 = 166(166) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+240 = 247(247) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+68 = 71(71) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 38+368 = 406(406) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+21 = 28(27) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 59+326 = 385(385) 1/0 1\n",
      "Epoch 20. Train Loss: 0.047794662, Test Loss : 0.038404886\n",
      "Epoch 21. Train Loss: 0.042539105, Test Loss : 0.0355625\n",
      "Epoch 22. Train Loss: 0.037586097, Test Loss : 0.04616268\n",
      "Epoch 23. Train Loss: 0.03386293, Test Loss : 0.043679215\n",
      "Epoch 24. Train Loss: 0.032564834, Test Loss : 0.16612928\n",
      "Epoch 25. Train Loss: 0.027551299, Test Loss : 0.0990245\n",
      "Epoch 26. Train Loss: 0.027917681, Test Loss : 0.02838631\n",
      "Epoch 27. Train Loss: 0.024123032, Test Loss : 0.022077117\n",
      "Epoch 28. Train Loss: 0.02111896, Test Loss : 0.115388215\n",
      "Epoch 29. Train Loss: 0.023385845, Test Loss : 0.096696846\n",
      "\u001b[92m☑\u001b[0m 874+74 = 948(948) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 591+8 = 699(599) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 8+9 = 178(17) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 55+42 = 97(97) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 97+214 = 311(311) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+727 = 731(731) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 902+8 = 910(910) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 62+237 = 299(299) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 74+5 = 80(79) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 4+92 = 97(96) 1/0 0\n",
      "Epoch 30. Train Loss: 0.020903064, Test Loss : 0.018523395\n",
      "Epoch 31. Train Loss: 0.01735679, Test Loss : 0.021558652\n",
      "Epoch 32. Train Loss: 0.017941779, Test Loss : 0.01737401\n",
      "Epoch 33. Train Loss: 0.015524957, Test Loss : 0.047953922\n",
      "Epoch 34. Train Loss: 0.01634989, Test Loss : 0.018600289\n",
      "Epoch 35. Train Loss: 0.013435951, Test Loss : 0.013250279\n",
      "Epoch 36. Train Loss: 0.016085701, Test Loss : 0.015072873\n",
      "Epoch 37. Train Loss: 0.011921856, Test Loss : 0.011142988\n",
      "Epoch 38. Train Loss: 0.013689282, Test Loss : 0.0206986\n",
      "Epoch 39. Train Loss: 0.0103301015, Test Loss : 0.02198454\n",
      "\u001b[92m☑\u001b[0m 245+5 = 250(250) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 11+46 = 57(57) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 13+0 = 14(13) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 3+89 = 92(92) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+8 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 598+45 = 643(643) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 374+3 = 377(377) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 158+69 = 227(227) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 32+1 = 33(33) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 79+12 = 91(91) 1/0 1\n",
      "Epoch 40. Train Loss: 0.012314442, Test Loss : 0.01751161\n",
      "Epoch 41. Train Loss: 0.010102631, Test Loss : 0.013667816\n",
      "Epoch 42. Train Loss: 0.009306052, Test Loss : 0.025580743\n",
      "Epoch 43. Train Loss: 0.011223659, Test Loss : 0.024335776\n",
      "Epoch 44. Train Loss: 0.009599633, Test Loss : 0.0131352935\n",
      "Epoch 45. Train Loss: 0.007909155, Test Loss : 0.019351605\n",
      "Epoch 46. Train Loss: 0.009508467, Test Loss : 0.010770479\n",
      "Epoch 47. Train Loss: 0.008873063, Test Loss : 0.097181015\n",
      "Epoch 48. Train Loss: 0.0078118104, Test Loss : 0.10873909\n",
      "Epoch 49. Train Loss: 0.0074116006, Test Loss : 0.018489301\n",
      "\u001b[92m☑\u001b[0m 213+22 = 235(235) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+14 = 24(14) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 626+311 = 937(937) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 51+26 = 77(77) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 123+78 = 191(201) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 3+254 = 257(257) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 5+6 = 119(11) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 66+45 = 111(111) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 348+68 = 416(416) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+8 = 16(16) 1/0 1\n",
      "Epoch 50. Train Loss: 0.006882738, Test Loss : 0.046344716\n",
      "Epoch 51. Train Loss: 0.0075785588, Test Loss : 0.0141373025\n",
      "Epoch 52. Train Loss: 0.008474424, Test Loss : 0.010002682\n",
      "Epoch 53. Train Loss: 0.004995934, Test Loss : 0.013000632\n",
      "Epoch 54. Train Loss: 0.0069138305, Test Loss : 0.050202616\n",
      "Epoch 55. Train Loss: 0.006078552, Test Loss : 0.011831162\n",
      "Epoch 56. Train Loss: 0.005861159, Test Loss : 0.016588513\n",
      "Epoch 57. Train Loss: 0.0049191476, Test Loss : 0.013170756\n",
      "Epoch 58. Train Loss: 0.0071131517, Test Loss : 0.01112415\n",
      "Epoch 59. Train Loss: 0.005102634, Test Loss : 0.009986306\n",
      "\u001b[92m☑\u001b[0m 4+267 = 271(271) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 76+2 = 89(78) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+370 = 375(375) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+330 = 333(333) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+6 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 194+142 = 336(336) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 314+311 = 615(625) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 2+502 = 504(504) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+9 = 173(13) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 264+372 = 636(636) 1/0 1\n",
      "Epoch 60. Train Loss: 0.0057636527, Test Loss : 0.011901276\n",
      "Epoch 61. Train Loss: 0.0047991574, Test Loss : 0.017314348\n",
      "Epoch 62. Train Loss: 0.0044775694, Test Loss : 0.012478552\n",
      "Epoch 63. Train Loss: 0.004141216, Test Loss : 0.01104085\n",
      "Epoch 64. Train Loss: 0.005165506, Test Loss : 0.022144452\n",
      "Epoch 65. Train Loss: 0.004698444, Test Loss : 0.013021069\n",
      "Epoch 66. Train Loss: 0.005515884, Test Loss : 0.011101652\n",
      "Epoch 67. Train Loss: 0.005052455, Test Loss : 0.009275643\n",
      "Epoch 68. Train Loss: 0.0033809857, Test Loss : 0.01918174\n",
      "Epoch 69. Train Loss: 0.003760817, Test Loss : 0.021372758\n",
      "\u001b[92m☑\u001b[0m 90+7 = 97(97) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 407+971 = 1378(1378) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 51+7 = 58(58) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 94+61 = 155(155) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+46 = 53(53) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+750 = 756(756) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 80+58 = 138(138) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 39+9 = 48(48) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 194+0 = 194(194) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 541+4 = 545(545) 1/0 1\n",
      "Epoch 70. Train Loss: 0.004422977, Test Loss : 0.013729341\n",
      "Epoch 71. Train Loss: 0.0030161163, Test Loss : 0.0133267\n",
      "Epoch 72. Train Loss: 0.0030300557, Test Loss : 0.009932427\n",
      "Epoch 73. Train Loss: 0.0040780175, Test Loss : 0.011157965\n",
      "Epoch 74. Train Loss: 0.0044595045, Test Loss : 0.0075438567\n",
      "Epoch 75. Train Loss: 0.002518032, Test Loss : 0.012633202\n",
      "Epoch 76. Train Loss: 0.005162913, Test Loss : 0.005555895\n",
      "Epoch 77. Train Loss: 0.002969341, Test Loss : 0.012750493\n",
      "Epoch 78. Train Loss: 0.004242709, Test Loss : 0.0059947195\n",
      "Epoch 79. Train Loss: 0.0032808369, Test Loss : 0.007413171\n",
      "\u001b[92m☑\u001b[0m 638+80 = 718(718) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 848+6 = 854(854) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 483+57 = 540(540) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+339 = 344(344) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+7 = 7(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 858+95 = 953(953) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 61+35 = 96(96) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 93+3 = 96(96) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+3 = 4(4) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 48+700 = 748(748) 1/0 1\n",
      "Epoch 80. Train Loss: 0.0029921138, Test Loss : 0.006291886\n",
      "Epoch 81. Train Loss: 0.0026494472, Test Loss : 0.008634965\n",
      "Epoch 82. Train Loss: 0.0033959702, Test Loss : 0.05027469\n",
      "Epoch 83. Train Loss: 0.0027617537, Test Loss : 0.009494452\n",
      "Epoch 84. Train Loss: 0.002630652, Test Loss : 0.0069599226\n",
      "Epoch 85. Train Loss: 0.003546034, Test Loss : 0.00739753\n",
      "Epoch 86. Train Loss: 0.0015096688, Test Loss : 0.034569003\n",
      "Epoch 87. Train Loss: 0.0040042703, Test Loss : 0.0070141912\n",
      "Epoch 88. Train Loss: 0.002180216, Test Loss : 0.008711212\n",
      "Epoch 89. Train Loss: 0.003089465, Test Loss : 0.012911478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m☑\u001b[0m 61+976 = 1037(1037) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+97 = 97(97) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+1 = 2(2) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+25 = 34(34) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 79+0 = 89(79) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 46+6 = 62(52) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 231+150 = 381(381) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+7 = 9(9) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 50+79 = 129(129) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 768+99 = 867(867) 1/0 1\n",
      "Epoch 90. Train Loss: 0.0027768165, Test Loss : 0.016044449\n",
      "Epoch 91. Train Loss: 0.0018821121, Test Loss : 0.006778906\n",
      "Epoch 92. Train Loss: 0.0036900977, Test Loss : 0.015098724\n",
      "Epoch 93. Train Loss: 0.002596438, Test Loss : 0.006146586\n",
      "Epoch 94. Train Loss: 0.0019872563, Test Loss : 0.0064983903\n",
      "Epoch 95. Train Loss: 0.0017905737, Test Loss : 0.008139258\n",
      "Epoch 96. Train Loss: 0.0016750622, Test Loss : 0.0075414246\n",
      "Epoch 97. Train Loss: 0.002756147, Test Loss : 0.00995801\n",
      "Epoch 98. Train Loss: 0.0017337408, Test Loss : 0.006174893\n",
      "Epoch 99. Train Loss: 0.0022295276, Test Loss : 0.011258623\n",
      "\u001b[92m☑\u001b[0m 8+8 = 16(16) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 38+648 = 686(686) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+55 = 58(58) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 98+598 = 696(696) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 29+25 = 54(54) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 75+91 = 166(166) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 32+9 = 41(41) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+570 = 571(571) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 247+80 = 327(327) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 897+52 = 949(949) 1/0 1\n",
      "Epoch 100. Train Loss: 0.0024368812, Test Loss : 0.009117115\n",
      "Epoch 101. Train Loss: 0.0019641193, Test Loss : 0.0042197886\n",
      "Epoch 102. Train Loss: 0.0019716553, Test Loss : 0.006239946\n",
      "Epoch 103. Train Loss: 0.001661554, Test Loss : 0.0065968363\n",
      "Epoch 104. Train Loss: 0.0017663427, Test Loss : 0.0068550548\n",
      "Epoch 105. Train Loss: 0.0016160134, Test Loss : 0.00633822\n",
      "Epoch 106. Train Loss: 0.0016446628, Test Loss : 0.0050523262\n",
      "Epoch 107. Train Loss: 0.0019469678, Test Loss : 0.006352355\n",
      "Epoch 108. Train Loss: 0.0021782042, Test Loss : 0.012054386\n",
      "Epoch 109. Train Loss: 0.0018712174, Test Loss : 0.009155972\n",
      "\u001b[91m☒\u001b[0m 69+2 = 81(71) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 955+76 = 1031(1031) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 883+0 = 883(883) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+3 = 8(8) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 56+4 = 60(60) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+100 = 105(105) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+2 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 57+26 = 83(83) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+8 = 8(8) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 241+519 = 760(760) 1/0 1\n",
      "Epoch 110. Train Loss: 0.0019318438, Test Loss : 0.007682926\n",
      "Epoch 111. Train Loss: 0.001425227, Test Loss : 0.12936053\n",
      "Epoch 112. Train Loss: 0.0019912852, Test Loss : 0.008244809\n",
      "Epoch 113. Train Loss: 0.0018501878, Test Loss : 0.005837693\n",
      "Epoch 114. Train Loss: 0.0017942843, Test Loss : 0.008839684\n",
      "Epoch 115. Train Loss: 0.0021935834, Test Loss : 0.017030263\n",
      "Epoch 116. Train Loss: 0.0022006081, Test Loss : 0.006115167\n",
      "Epoch 117. Train Loss: 0.0015257987, Test Loss : 0.004309995\n",
      "Epoch 118. Train Loss: 0.0010097043, Test Loss : 0.0072167926\n",
      "Epoch 119. Train Loss: 0.0010072503, Test Loss : 0.004339005\n",
      "\u001b[92m☑\u001b[0m 5+60 = 65(65) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+4 = 109(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 35+767 = 802(802) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+64 = 70(70) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 747+47 = 794(794) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 586+920 = 1406(1506) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 156+596 = 752(752) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+2 = 6(6) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+23 = 28(27) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 5+6 = 119(11) 1/0 0\n",
      "Epoch 120. Train Loss: 0.0005729526, Test Loss : 0.004453237\n",
      "Epoch 121. Train Loss: 0.0018162795, Test Loss : 0.0059494767\n",
      "Epoch 122. Train Loss: 0.00036403316, Test Loss : 0.0116728665\n",
      "Epoch 123. Train Loss: 0.00124618, Test Loss : 0.007059577\n",
      "Epoch 124. Train Loss: 0.0018592507, Test Loss : 0.0074348026\n",
      "Epoch 125. Train Loss: 0.0011900326, Test Loss : 0.012391696\n",
      "Epoch 126. Train Loss: 0.002229247, Test Loss : 0.004805271\n",
      "Epoch 127. Train Loss: 0.00028743112, Test Loss : 0.00906604\n",
      "Epoch 128. Train Loss: 0.0010278919, Test Loss : 0.0056593744\n",
      "Epoch 129. Train Loss: 8.564001e-05, Test Loss : 0.0041464353\n",
      "\u001b[92m☑\u001b[0m 7+8 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 970+53 = 1023(1023) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 82+7 = 98(89) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+606 = 614(614) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 997+6 = 1003(1003) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 222+7 = 229(229) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 16+18 = 34(34) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 969+60 = 1029(1029) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+73 = 81(81) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 94+6 = 109(100) 1/0 0\n",
      "Epoch 130. Train Loss: 1.6295626e-05, Test Loss : 0.0038394243\n",
      "Epoch 131. Train Loss: 1.1607818e-05, Test Loss : 0.0034440607\n",
      "Epoch 132. Train Loss: 9.550524e-06, Test Loss : 0.0033263988\n",
      "Epoch 133. Train Loss: 8.558288e-06, Test Loss : 0.0033592246\n",
      "Epoch 134. Train Loss: 7.747101e-06, Test Loss : 0.0033923127\n",
      "Epoch 135. Train Loss: 7.255207e-06, Test Loss : 0.0033228993\n",
      "Epoch 136. Train Loss: 6.853465e-06, Test Loss : 0.0033095542\n",
      "Epoch 137. Train Loss: 6.437802e-06, Test Loss : 0.0033277199\n",
      "Epoch 138. Train Loss: 6.2820286e-06, Test Loss : 0.0034637037\n",
      "Epoch 139. Train Loss: 5.928769e-06, Test Loss : 0.0033174164\n",
      "\u001b[92m☑\u001b[0m 8+95 = 103(103) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+7 = 13(13) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+352 = 350(360) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 875+46 = 1221(921) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+74 = 80(80) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+2 = 7(7) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 69+7 = 75(76) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 926+672 = 1598(1598) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 542+0 = 542(542) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 131+486 = 617(617) 1/0 1\n",
      "Epoch 140. Train Loss: 5.6106655e-06, Test Loss : 0.0032627631\n",
      "Epoch 141. Train Loss: 5.469777e-06, Test Loss : 0.0033523045\n",
      "Epoch 142. Train Loss: 5.2402856e-06, Test Loss : 0.003188604\n",
      "Epoch 143. Train Loss: 5.4082843e-06, Test Loss : 0.0032391616\n",
      "Epoch 144. Train Loss: 5.0024264e-06, Test Loss : 0.00369208\n",
      "Epoch 145. Train Loss: 4.7903945e-06, Test Loss : 0.00333139\n",
      "Epoch 146. Train Loss: 4.703266e-06, Test Loss : 0.0033856642\n",
      "Epoch 147. Train Loss: 4.543705e-06, Test Loss : 0.0032319285\n",
      "Epoch 148. Train Loss: 4.4541625e-06, Test Loss : 0.0032402999\n",
      "Epoch 149. Train Loss: 4.3098757e-06, Test Loss : 0.0035502657\n",
      "\u001b[92m☑\u001b[0m 99+7 = 106(106) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+681 = 690(690) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+4 = 8(8) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+10 = 19(19) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+85 = 92(92) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+76 = 78(78) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 93+783 = 876(876) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 55+46 = 101(101) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+9 = 10(10) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+70 = 79(79) 1/0 1\n",
      "Epoch 150. Train Loss: 4.108811e-06, Test Loss : 0.0034702264\n",
      "Epoch 151. Train Loss: 4.181888e-06, Test Loss : 0.0031820394\n",
      "Epoch 152. Train Loss: 4.0617483e-06, Test Loss : 0.003290762\n",
      "Epoch 153. Train Loss: 3.9492497e-06, Test Loss : 0.0032276877\n",
      "Epoch 154. Train Loss: 3.822756e-06, Test Loss : 0.0032262374\n",
      "Epoch 155. Train Loss: 3.8004741e-06, Test Loss : 0.0032614481\n",
      "Epoch 156. Train Loss: 3.7204643e-06, Test Loss : 0.0032038235\n",
      "Epoch 157. Train Loss: 3.6289682e-06, Test Loss : 0.0033079884\n",
      "Epoch 158. Train Loss: 3.5418716e-06, Test Loss : 0.0031812552\n",
      "Epoch 159. Train Loss: 3.4903692e-06, Test Loss : 0.003228347\n",
      "\u001b[92m☑\u001b[0m 783+3 = 786(786) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+3 = 12(12) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 22+0 = 22(22) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+2 = 2(2) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 466+70 = 536(536) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 910+4 = 914(914) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+5 = 119(11) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 342+36 = 378(378) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+1 = 2(2) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 40+1 = 41(41) 1/0 1\n",
      "Epoch 160. Train Loss: 3.4975276e-06, Test Loss : 0.003151487\n",
      "Epoch 161. Train Loss: 3.3494916e-06, Test Loss : 0.003154594\n",
      "Epoch 162. Train Loss: 3.3236402e-06, Test Loss : 0.0032137383\n",
      "Epoch 163. Train Loss: 3.287597e-06, Test Loss : 0.0031595372\n",
      "Epoch 164. Train Loss: 3.1511577e-06, Test Loss : 0.0031548117\n",
      "Epoch 165. Train Loss: 3.2219484e-06, Test Loss : 0.0035041522\n",
      "Epoch 166. Train Loss: 3.1493203e-06, Test Loss : 0.0032527861\n",
      "Epoch 167. Train Loss: 3.138774e-06, Test Loss : 0.003251632\n",
      "Epoch 168. Train Loss: 2.990457e-06, Test Loss : 0.0032846797\n",
      "Epoch 169. Train Loss: 2.9957935e-06, Test Loss : 0.0034797483\n",
      "\u001b[92m☑\u001b[0m 3+487 = 490(490) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 311+400 = 711(711) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 663+33 = 696(696) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 30+302 = 332(332) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+1 = 20(9) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 4+0 = 54(4) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 17+13 = 30(30) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 66+2 = 68(68) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 99+87 = 176(186) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 4+795 = 799(799) 1/0 1\n",
      "Epoch 170. Train Loss: 2.919548e-06, Test Loss : 0.0031782624\n",
      "Epoch 171. Train Loss: 2.9227326e-06, Test Loss : 0.0031639717\n",
      "Epoch 172. Train Loss: 2.9182381e-06, Test Loss : 0.0032740715\n",
      "Epoch 173. Train Loss: 2.8368945e-06, Test Loss : 0.00339064\n",
      "Epoch 174. Train Loss: 2.7648205e-06, Test Loss : 0.003166704\n",
      "Epoch 175. Train Loss: 2.757797e-06, Test Loss : 0.0031430232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176. Train Loss: 2.7230071e-06, Test Loss : 0.0032036384\n",
      "Epoch 177. Train Loss: 2.6737152e-06, Test Loss : 0.0033535666\n",
      "Epoch 178. Train Loss: 2.6182997e-06, Test Loss : 0.0032702852\n",
      "Epoch 179. Train Loss: 2.6338332e-06, Test Loss : 0.003262053\n",
      "\u001b[92m☑\u001b[0m 49+42 = 91(91) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 357+99 = 456(456) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+80 = 88(87) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 415+79 = 494(494) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 770+4 = 774(774) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 52+59 = 111(111) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 233+4 = 237(237) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+6 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 34+33 = 67(67) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 54+88 = 142(142) 1/0 1\n",
      "Epoch 180. Train Loss: 2.621395e-06, Test Loss : 0.0031281826\n",
      "Epoch 181. Train Loss: 2.5484942e-06, Test Loss : 0.0034650187\n",
      "Epoch 182. Train Loss: 2.5050192e-06, Test Loss : 0.003182826\n",
      "Epoch 183. Train Loss: 2.4409676e-06, Test Loss : 0.0033390813\n",
      "Epoch 184. Train Loss: 2.4870913e-06, Test Loss : 0.0032807663\n",
      "Epoch 185. Train Loss: 2.4206333e-06, Test Loss : 0.0032118172\n",
      "Epoch 186. Train Loss: 2.4492713e-06, Test Loss : 0.003574138\n",
      "Epoch 187. Train Loss: 2.4270032e-06, Test Loss : 0.003264118\n",
      "Epoch 188. Train Loss: 2.338675e-06, Test Loss : 0.0032693613\n",
      "Epoch 189. Train Loss: 2.3154405e-06, Test Loss : 0.0034365035\n",
      "\u001b[92m☑\u001b[0m 39+217 = 256(256) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 386+388 = 774(774) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+2 = 3(3) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 644+97 = 741(741) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 91+8 = 107(99) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+23 = 34(24) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 27+65 = 92(92) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 42+6 = 48(48) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 109+11 = 120(120) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 22+975 = 997(997) 1/0 1\n",
      "Epoch 190. Train Loss: 2.3159555e-06, Test Loss : 0.0031499502\n",
      "Epoch 191. Train Loss: 2.2634385e-06, Test Loss : 0.0034084655\n",
      "Epoch 192. Train Loss: 2.2176534e-06, Test Loss : 0.0031586823\n",
      "Epoch 193. Train Loss: 2.267643e-06, Test Loss : 0.0031718728\n",
      "Epoch 194. Train Loss: 2.1986548e-06, Test Loss : 0.0033440539\n",
      "Epoch 195. Train Loss: 2.1474611e-06, Test Loss : 0.003191508\n",
      "Epoch 196. Train Loss: 2.162382e-06, Test Loss : 0.0031912983\n",
      "Epoch 197. Train Loss: 2.0931102e-06, Test Loss : 0.0033755875\n",
      "Epoch 198. Train Loss: 2.1366325e-06, Test Loss : 0.0031567886\n",
      "Epoch 199. Train Loss: 2.1690064e-06, Test Loss : 0.0035227411\n",
      "\u001b[92m☑\u001b[0m 1+461 = 462(462) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+1 = 20(9) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 54+2 = 57(56) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 75+65 = 140(140) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 14+9 = 22(23) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+6 = 14(14) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+453 = 462(462) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 27+60 = 87(87) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 273+44 = 317(317) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 85+898 = 983(983) 1/0 1\n",
      "Epoch 200. Train Loss: 2.0871373e-06, Test Loss : 0.0035018884\n"
     ]
    }
   ],
   "source": [
    "epochs = 201\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p = model.calculation(q[i], char_indices, indices_char).strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) 1/0 {}\".format(q[i], p, y[i], str(iscorr) ))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
