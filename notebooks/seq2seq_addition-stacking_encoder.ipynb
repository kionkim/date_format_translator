{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM vs LSTMCell\n",
    "\n",
    "  * LSTM과 LSTMCell은 서로 다른 형태의 parameter를 지님\n",
    "  * LSTMCell은 1번의 time step을 도는 것을 가정, LSTM은 주어진 timestep 모두를 도는 것으로 가정\n",
    "  \n",
    "  * LSTM의 state parameter는 (num_layer, batch_size, n_hidden)/ LSTMCell의 state parameter는 (batch_size, n_hidden)을 지님\n",
    "  * LSTM의 state는 time step만큼의 hidden state값을 모두 포함, LSTMCell의 state는 1 time step만큼의 hidden state값만 포함\n",
    "  * LSTMCell의 unroll을 쓰면 \n",
    "  * Encoder에 LSTM을 쓰고, Decoder에 LSTMCell을 쓰는 경우에는 state parameter에 유의해야 함\n",
    "      * Encoder에 LSTM을 쓰는 이유는 stacking을 위해서이고, Decoder에 LSTMCell을 쓰는 이유는 generation을 위해서임.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, enc_layer, dec_layer = 1, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_layer = enc_layer\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTM(hidden_size = n_hidden, num_layers = enc_layer, layout = 'NTC')\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        # API says: num_layers, batch_size, num_hidden\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        begin_state = self.encoder.begin_state(batch_size = self.batch_size, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(inputs, begin_state) # h, c: n_layer * batch_size * n_hidden\n",
    "        # Pick the hidden states and cell states at the last time step in the second layer\n",
    "        next_h = h[1] # batch_size * n_hidden\n",
    "        next_c = c[1] # batch_size * n_hidden\n",
    "        #next_h = nd.mean(h, axis = 0) #: Does not work\n",
    "        #next_c = nd.mean(c, axis = 0) # Does not work\n",
    "        for i in range(self.out_seq_len):\n",
    "\n",
    "            deout, (next_h, next_c) = self.decoder(outputs[:, i, :], [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)\n",
    "        \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculation(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        begin_state = self.encoder.begin_state(batch_size = 1, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(X, begin_state)\n",
    "        next_h = h[1]\n",
    "        next_c = c[1]\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(deout, [next_h, next_c])\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "        return ret_seq.strip('E').strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14, 2)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTM(None -> 300, NTC, num_layers=2)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 87+8 = 102(95) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 45+0 = 524(45) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 28+3 = 138(31) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+50 = 102(50) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 265+84 = 663(349) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 40+948 = 1020(988) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 27+1 = 124(28) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 37+623 = 738(660) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 8+2 = 102(10) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 7+0 = 102(7) 1/0 0\n",
      "Epoch 0. Train Loss: 1.1953808, Test Loss : 1.1294714\n",
      "Epoch 1. Train Loss: 1.12513, Test Loss : 1.1277721\n",
      "Epoch 2. Train Loss: 1.104025, Test Loss : 1.0783864\n",
      "Epoch 3. Train Loss: 1.0531863, Test Loss : 0.99584687\n",
      "Epoch 4. Train Loss: 0.93822825, Test Loss : 0.8749555\n",
      "Epoch 5. Train Loss: 0.8466254, Test Loss : 0.815833\n",
      "Epoch 6. Train Loss: 0.76491624, Test Loss : 0.7248025\n",
      "Epoch 7. Train Loss: 0.67228866, Test Loss : 0.6715063\n",
      "Epoch 8. Train Loss: 0.5836759, Test Loss : 0.5467197\n",
      "Epoch 9. Train Loss: 0.48728725, Test Loss : 0.4945925\n",
      "\u001b[91m☒\u001b[0m 6+57 = 64(63) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 745+656 = 1412(1401) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 84+0 = 96(84) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 5+15 = 199(20) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+73 = 73(74) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 47+41 = 87(88) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 3+57 = 61(60) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 18+504 = 522(522) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+7 = 87(7) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+287 = 292(292) 1/0 1\n",
      "Epoch 10. Train Loss: 0.3493879, Test Loss : 0.30299848\n",
      "Epoch 11. Train Loss: 0.23558478, Test Loss : 0.18327525\n",
      "Epoch 12. Train Loss: 0.16363388, Test Loss : 0.17315158\n",
      "Epoch 13. Train Loss: 0.12062051, Test Loss : 0.08230791\n",
      "Epoch 14. Train Loss: 0.09442894, Test Loss : 0.06338678\n",
      "Epoch 15. Train Loss: 0.07407126, Test Loss : 0.04219369\n",
      "Epoch 16. Train Loss: 0.061343644, Test Loss : 0.09977138\n",
      "Epoch 17. Train Loss: 0.05425773, Test Loss : 0.208276\n",
      "Epoch 18. Train Loss: 0.04689556, Test Loss : 0.09927837\n",
      "Epoch 19. Train Loss: 0.04202856, Test Loss : 0.15873668\n",
      "\u001b[91m☒\u001b[0m 5+9 = 143(14) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 43+2 = 45(45) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+20 = 21(22) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 565+16 = 581(581) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 37+4 = 51(41) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 0+556 = 556(556) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 99+7 = 106(106) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+127 = 130(130) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 58+62 = 129(120) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 61+60 = 121(121) 1/0 1\n",
      "Epoch 20. Train Loss: 0.03429959, Test Loss : 0.025102835\n",
      "Epoch 21. Train Loss: 0.032527775, Test Loss : 0.023400802\n",
      "Epoch 22. Train Loss: 0.027673082, Test Loss : 0.018558353\n",
      "Epoch 23. Train Loss: 0.025122691, Test Loss : 0.03364276\n",
      "Epoch 24. Train Loss: 0.022873705, Test Loss : 0.014064267\n",
      "Epoch 25. Train Loss: 0.020519434, Test Loss : 0.011598343\n",
      "Epoch 26. Train Loss: 0.019085757, Test Loss : 0.015565535\n",
      "Epoch 27. Train Loss: 0.017218791, Test Loss : 0.010602449\n",
      "Epoch 28. Train Loss: 0.016813284, Test Loss : 0.014435778\n",
      "Epoch 29. Train Loss: 0.014339938, Test Loss : 0.00930839\n",
      "\u001b[92m☑\u001b[0m 234+35 = 269(269) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 610+14 = 624(624) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 566+545 = 1111(1111) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 86+2 = 98(88) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 82+3 = 96(85) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+187 = 192(192) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 56+9 = 655(65) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 685+23 = 708(708) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+69 = 70(71) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 274+5 = 279(279) 1/0 1\n",
      "Epoch 30. Train Loss: 0.012468022, Test Loss : 0.009633763\n",
      "Epoch 31. Train Loss: 0.01621592, Test Loss : 0.011579801\n",
      "Epoch 32. Train Loss: 0.012712877, Test Loss : 0.007253586\n",
      "Epoch 33. Train Loss: 0.010858403, Test Loss : 0.013775451\n",
      "Epoch 34. Train Loss: 0.011124798, Test Loss : 0.011595459\n",
      "Epoch 35. Train Loss: 0.01020459, Test Loss : 0.007988406\n",
      "Epoch 36. Train Loss: 0.0096490495, Test Loss : 0.037054587\n",
      "Epoch 37. Train Loss: 0.0052781887, Test Loss : 0.0071145766\n",
      "Epoch 38. Train Loss: 0.010722455, Test Loss : 0.0067159245\n",
      "Epoch 39. Train Loss: 0.00929148, Test Loss : 0.0061739474\n",
      "\u001b[92m☑\u001b[0m 128+5 = 133(133) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 18+65 = 83(83) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 146+5 = 151(151) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 112+24 = 136(136) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 33+55 = 88(88) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+244 = 246(246) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 24+26 = 50(50) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 49+152 = 201(201) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 497+513 = 1010(1010) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 41+856 = 897(897) 1/0 1\n",
      "Epoch 40. Train Loss: 0.006686136, Test Loss : 0.007074114\n",
      "Epoch 41. Train Loss: 0.0097831, Test Loss : 0.0050642304\n",
      "Epoch 42. Train Loss: 0.0058348044, Test Loss : 0.009020301\n",
      "Epoch 43. Train Loss: 0.008660523, Test Loss : 0.0071797087\n",
      "Epoch 44. Train Loss: 0.004568754, Test Loss : 0.0058556898\n",
      "Epoch 45. Train Loss: 0.005024118, Test Loss : 0.005002115\n",
      "Epoch 46. Train Loss: 0.0072087734, Test Loss : 0.015023458\n",
      "Epoch 47. Train Loss: 0.0066026165, Test Loss : 0.0073885047\n",
      "Epoch 48. Train Loss: 0.006915657, Test Loss : 0.0075922525\n",
      "Epoch 49. Train Loss: 0.0038078008, Test Loss : 0.0072693424\n",
      "\u001b[92m☑\u001b[0m 554+279 = 833(833) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+210 = 210(210) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 63+1 = 64(64) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 92+72 = 164(164) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 83+6 = 99(89) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 78+598 = 676(676) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 43+39 = 82(82) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 132+805 = 937(937) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 62+677 = 739(739) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 36+329 = 365(365) 1/0 1\n",
      "Epoch 50. Train Loss: 0.0057752486, Test Loss : 0.038065664\n",
      "Epoch 51. Train Loss: 0.0056857644, Test Loss : 0.009097111\n",
      "Epoch 52. Train Loss: 0.0050005517, Test Loss : 0.007037791\n",
      "Epoch 53. Train Loss: 0.0044671725, Test Loss : 0.010285665\n",
      "Epoch 54. Train Loss: 0.006079741, Test Loss : 0.005524069\n",
      "Epoch 55. Train Loss: 0.003852714, Test Loss : 0.005184916\n",
      "Epoch 56. Train Loss: 0.003830175, Test Loss : 0.012649392\n",
      "Epoch 57. Train Loss: 0.0049141427, Test Loss : 0.0036504765\n",
      "Epoch 58. Train Loss: 0.0040175263, Test Loss : 0.004806501\n",
      "Epoch 59. Train Loss: 0.004017754, Test Loss : 0.0047264723\n",
      "\u001b[92m☑\u001b[0m 244+57 = 301(301) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 37+24 = 61(61) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 626+91 = 717(717) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+3 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+283 = 292(292) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 138+9 = 147(147) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 80+582 = 662(662) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 95+99 = 193(194) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 952+748 = 1600(1700) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 29+444 = 473(473) 1/0 1\n",
      "Epoch 60. Train Loss: 0.0032264022, Test Loss : 0.007288846\n",
      "Epoch 61. Train Loss: 0.0025689772, Test Loss : 0.0029573946\n",
      "Epoch 62. Train Loss: 8.5216634e-05, Test Loss : 0.0021961604\n",
      "Epoch 63. Train Loss: 3.823611e-05, Test Loss : 0.0023411345\n",
      "Epoch 64. Train Loss: 2.9950148e-05, Test Loss : 0.0019830188\n",
      "Epoch 65. Train Loss: 2.558636e-05, Test Loss : 0.0019599292\n",
      "Epoch 66. Train Loss: 2.2624976e-05, Test Loss : 0.002041954\n",
      "Epoch 67. Train Loss: 2.0716172e-05, Test Loss : 0.0019399896\n",
      "Epoch 68. Train Loss: 1.8938254e-05, Test Loss : 0.0019955575\n",
      "Epoch 69. Train Loss: 1.7265917e-05, Test Loss : 0.001893807\n",
      "\u001b[92m☑\u001b[0m 7+77 = 84(84) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+24 = 31(31) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 877+5 = 882(882) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 263+7 = 270(270) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 60+93 = 153(153) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+44 = 46(46) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 780+58 = 838(838) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 599+29 = 628(628) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 471+366 = 837(837) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 47+4 = 51(51) 1/0 1\n",
      "Epoch 70. Train Loss: 1.6123515e-05, Test Loss : 0.0018791116\n",
      "Epoch 71. Train Loss: 1.5239403e-05, Test Loss : 0.002005206\n",
      "Epoch 72. Train Loss: 1.4100053e-05, Test Loss : 0.0019343406\n",
      "Epoch 73. Train Loss: 1.3410352e-05, Test Loss : 0.0018950151\n",
      "Epoch 74. Train Loss: 1.2731963e-05, Test Loss : 0.0019278148\n",
      "Epoch 75. Train Loss: 1.2176382e-05, Test Loss : 0.0019073443\n",
      "Epoch 76. Train Loss: 1.1349483e-05, Test Loss : 0.0018882917\n",
      "Epoch 77. Train Loss: 1.1050649e-05, Test Loss : 0.0018719316\n",
      "Epoch 78. Train Loss: 1.0583219e-05, Test Loss : 0.0018359132\n",
      "Epoch 79. Train Loss: 1.0301651e-05, Test Loss : 0.0018592796\n",
      "\u001b[92m☑\u001b[0m 33+7 = 40(40) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 25+632 = 657(657) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 24+2 = 36(26) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 85+7 = 923(92) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 516+265 = 781(781) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+6 = 7(7) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+6 = 8(9) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 3+90 = 923(93) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+732 = 738(738) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 550+848 = 1398(1398) 1/0 1\n",
      "Epoch 80. Train Loss: 9.62994e-06, Test Loss : 0.0018737955\n",
      "Epoch 81. Train Loss: 9.400475e-06, Test Loss : 0.0018679667\n",
      "Epoch 82. Train Loss: 9.0191115e-06, Test Loss : 0.0019931472\n",
      "Epoch 83. Train Loss: 8.635378e-06, Test Loss : 0.0018535357\n",
      "Epoch 84. Train Loss: 8.340753e-06, Test Loss : 0.0018552269\n",
      "Epoch 85. Train Loss: 8.210077e-06, Test Loss : 0.0018488595\n",
      "Epoch 86. Train Loss: 7.854931e-06, Test Loss : 0.00183795\n",
      "Epoch 87. Train Loss: 7.770975e-06, Test Loss : 0.0018664645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88. Train Loss: 7.532723e-06, Test Loss : 0.0018578371\n",
      "Epoch 89. Train Loss: 7.2334424e-06, Test Loss : 0.001878264\n",
      "\u001b[92m☑\u001b[0m 28+39 = 67(67) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 90+51 = 141(141) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 799+0 = 799(799) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 12+28 = 40(40) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 11+7 = 18(18) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 55+88 = 143(143) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 10+5 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+369 = 378(378) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 167+1 = 168(168) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 500+26 = 526(526) 1/0 1\n",
      "Epoch 90. Train Loss: 7.1406625e-06, Test Loss : 0.0018877139\n",
      "Epoch 91. Train Loss: 6.967133e-06, Test Loss : 0.0018468521\n",
      "Epoch 92. Train Loss: 6.725182e-06, Test Loss : 0.0018626777\n",
      "Epoch 93. Train Loss: 6.4540463e-06, Test Loss : 0.0018544985\n",
      "Epoch 94. Train Loss: 6.3536236e-06, Test Loss : 0.0018264944\n",
      "Epoch 95. Train Loss: 6.224529e-06, Test Loss : 0.0018367938\n",
      "Epoch 96. Train Loss: 6.0399834e-06, Test Loss : 0.0018573438\n",
      "Epoch 97. Train Loss: 5.9027107e-06, Test Loss : 0.0018553918\n",
      "Epoch 98. Train Loss: 5.714528e-06, Test Loss : 0.0018578762\n",
      "Epoch 99. Train Loss: 5.5463443e-06, Test Loss : 0.0018181864\n",
      "\u001b[92m☑\u001b[0m 1+216 = 217(217) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+21 = 27(27) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 81+158 = 239(239) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 14+64 = 77(78) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 603+6 = 609(609) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 793+4 = 797(797) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+478 = 485(485) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+43 = 44(45) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 7+26 = 33(33) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+512 = 515(515) 1/0 1\n",
      "Epoch 100. Train Loss: 5.5284863e-06, Test Loss : 0.0018364536\n",
      "Epoch 101. Train Loss: 5.498123e-06, Test Loss : 0.0018059525\n",
      "Epoch 102. Train Loss: 5.266919e-06, Test Loss : 0.0018212402\n",
      "Epoch 103. Train Loss: 5.2141418e-06, Test Loss : 0.0019176921\n",
      "Epoch 104. Train Loss: 5.1834854e-06, Test Loss : 0.0018025383\n",
      "Epoch 105. Train Loss: 4.9868904e-06, Test Loss : 0.0018168878\n",
      "Epoch 106. Train Loss: 4.914056e-06, Test Loss : 0.0018174313\n",
      "Epoch 107. Train Loss: 4.838586e-06, Test Loss : 0.0018383799\n",
      "Epoch 108. Train Loss: 4.7774793e-06, Test Loss : 0.0019985377\n",
      "Epoch 109. Train Loss: 4.6747004e-06, Test Loss : 0.0018121589\n",
      "\u001b[92m☑\u001b[0m 94+75 = 169(169) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+408 = 413(413) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 505+386 = 891(891) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 34+0 = 44(34) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 46+114 = 160(160) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+83 = 90(90) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 365+50 = 415(415) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+856 = 860(860) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+298 = 306(306) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 70+482 = 552(552) 1/0 1\n",
      "Epoch 110. Train Loss: 4.560761e-06, Test Loss : 0.0018139975\n",
      "Epoch 111. Train Loss: 4.4846e-06, Test Loss : 0.0017885554\n",
      "Epoch 112. Train Loss: 4.4126828e-06, Test Loss : 0.0020513504\n",
      "Epoch 113. Train Loss: 4.3498394e-06, Test Loss : 0.0018529054\n",
      "Epoch 114. Train Loss: 4.2948045e-06, Test Loss : 0.0018485533\n",
      "Epoch 115. Train Loss: 4.153641e-06, Test Loss : 0.0018394424\n",
      "Epoch 116. Train Loss: 4.051645e-06, Test Loss : 0.0019520952\n",
      "Epoch 117. Train Loss: 4.0305367e-06, Test Loss : 0.0019289672\n",
      "Epoch 118. Train Loss: 3.987389e-06, Test Loss : 0.0018641163\n",
      "Epoch 119. Train Loss: 3.9536767e-06, Test Loss : 0.0018300635\n",
      "\u001b[92m☑\u001b[0m 284+146 = 430(430) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+280 = 281(281) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 909+55 = 964(964) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+3 = 32(3) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 9+66 = 75(75) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+681 = 682(682) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+88 = 91(91) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+684 = 692(692) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 495+49 = 544(544) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 31+60 = 92(91) 1/0 0\n",
      "Epoch 120. Train Loss: 3.8765993e-06, Test Loss : 0.0018189261\n",
      "Epoch 121. Train Loss: 3.8322582e-06, Test Loss : 0.0019439291\n",
      "Epoch 122. Train Loss: 3.7898783e-06, Test Loss : 0.0017993683\n",
      "Epoch 123. Train Loss: 3.7834316e-06, Test Loss : 0.0018341818\n",
      "Epoch 124. Train Loss: 3.6887611e-06, Test Loss : 0.001862824\n",
      "Epoch 125. Train Loss: 3.569262e-06, Test Loss : 0.0018257282\n",
      "Epoch 126. Train Loss: 3.546165e-06, Test Loss : 0.0018341253\n",
      "Epoch 127. Train Loss: 3.606484e-06, Test Loss : 0.0018411577\n",
      "Epoch 128. Train Loss: 3.4803465e-06, Test Loss : 0.0018187445\n",
      "Epoch 129. Train Loss: 3.4334873e-06, Test Loss : 0.0021441698\n",
      "\u001b[92m☑\u001b[0m 13+90 = 103(103) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+2 = 22(2) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 203+95 = 298(298) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 220+4 = 224(224) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 202+47 = 259(249) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+83 = 83(84) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 41+84 = 125(125) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 93+157 = 250(250) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 74+8 = 95(82) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 10+120 = 130(130) 1/0 1\n",
      "Epoch 130. Train Loss: 3.4090037e-06, Test Loss : 0.001792528\n",
      "Epoch 131. Train Loss: 3.24968e-06, Test Loss : 0.0018456677\n",
      "Epoch 132. Train Loss: 3.2394703e-06, Test Loss : 0.001762721\n",
      "Epoch 133. Train Loss: 3.2096345e-06, Test Loss : 0.0018215775\n",
      "Epoch 134. Train Loss: 3.229326e-06, Test Loss : 0.0019021003\n",
      "Epoch 135. Train Loss: 3.1474935e-06, Test Loss : 0.0018003331\n",
      "Epoch 136. Train Loss: 3.122056e-06, Test Loss : 0.0018392156\n",
      "Epoch 137. Train Loss: 3.178519e-06, Test Loss : 0.0018041848\n",
      "Epoch 138. Train Loss: 3.0391493e-06, Test Loss : 0.0018026965\n",
      "Epoch 139. Train Loss: 3.068553e-06, Test Loss : 0.002232865\n",
      "\u001b[91m☒\u001b[0m 9+4 = 53(13) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 37+1 = 38(38) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 731+9 = 740(740) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+5 = 8(8) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 717+563 = 1280(1280) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 754+30 = 784(784) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 571+9 = 570(580) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 38+931 = 969(969) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 13+1 = 24(14) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 8+33 = 42(41) 1/0 0\n",
      "Epoch 140. Train Loss: 3.0445299e-06, Test Loss : 0.0019672662\n",
      "Epoch 141. Train Loss: 2.980008e-06, Test Loss : 0.0018050643\n",
      "Epoch 142. Train Loss: 2.8959926e-06, Test Loss : 0.0019431838\n",
      "Epoch 143. Train Loss: 2.856509e-06, Test Loss : 0.0018730743\n",
      "Epoch 144. Train Loss: 2.8160628e-06, Test Loss : 0.0018335339\n",
      "Epoch 145. Train Loss: 2.8172906e-06, Test Loss : 0.001936738\n",
      "Epoch 146. Train Loss: 2.7571573e-06, Test Loss : 0.0018227035\n",
      "Epoch 147. Train Loss: 2.7369767e-06, Test Loss : 0.0018092568\n",
      "Epoch 148. Train Loss: 2.7295387e-06, Test Loss : 0.0017985506\n",
      "Epoch 149. Train Loss: 2.6826685e-06, Test Loss : 0.0019128912\n",
      "\u001b[92m☑\u001b[0m 73+92 = 165(165) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 468+900 = 1368(1368) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 195+3 = 198(198) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 64+3 = 67(67) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+6 = 91(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 92+79 = 171(171) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 33+629 = 662(662) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+27 = 32(32) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+9 = 90(9) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 13+7 = 20(20) 1/0 1\n",
      "Epoch 150. Train Loss: 2.6612538e-06, Test Loss : 0.0018098599\n",
      "Epoch 151. Train Loss: 2.6128948e-06, Test Loss : 0.0017645849\n",
      "Epoch 152. Train Loss: 2.667363e-06, Test Loss : 0.0018113274\n",
      "Epoch 153. Train Loss: 2.5962192e-06, Test Loss : 0.0017639281\n",
      "Epoch 154. Train Loss: 2.5965512e-06, Test Loss : 0.0017908299\n",
      "Epoch 155. Train Loss: 2.5342365e-06, Test Loss : 0.0018360186\n",
      "Epoch 156. Train Loss: 2.4772305e-06, Test Loss : 0.001792659\n",
      "Epoch 157. Train Loss: 2.4977237e-06, Test Loss : 0.0019273311\n",
      "Epoch 158. Train Loss: 2.5136692e-06, Test Loss : 0.0017989827\n",
      "Epoch 159. Train Loss: 2.5012473e-06, Test Loss : 0.0018468231\n",
      "\u001b[92m☑\u001b[0m 56+896 = 952(952) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+10 = 12(13) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 7+19 = 27(26) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+13 = 29(19) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 192+5 = 197(197) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 23+660 = 683(683) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 88+4 = 112(92) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 466+47 = 513(513) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 71+3 = 74(74) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 226+634 = 860(860) 1/0 1\n",
      "Epoch 160. Train Loss: 2.3504717e-06, Test Loss : 0.0018108394\n",
      "Epoch 161. Train Loss: 2.3844457e-06, Test Loss : 0.0018998059\n",
      "Epoch 162. Train Loss: 2.4133535e-06, Test Loss : 0.0017943572\n",
      "Epoch 163. Train Loss: 2.326252e-06, Test Loss : 0.0017850433\n",
      "Epoch 164. Train Loss: 2.3442465e-06, Test Loss : 0.0018235382\n",
      "Epoch 165. Train Loss: 2.3337657e-06, Test Loss : 0.0018484101\n",
      "Epoch 166. Train Loss: 2.303372e-06, Test Loss : 0.0018116668\n",
      "Epoch 167. Train Loss: 2.2684208e-06, Test Loss : 0.0018838231\n",
      "Epoch 168. Train Loss: 2.2090558e-06, Test Loss : 0.0018139642\n",
      "Epoch 169. Train Loss: 2.1990616e-06, Test Loss : 0.0019773445\n",
      "\u001b[92m☑\u001b[0m 122+105 = 227(227) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 953+972 = 1925(1925) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 365+188 = 553(553) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+342 = 350(350) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 37+79 = 116(116) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+71 = 76(75) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 3+3 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 918+1 = 919(919) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 9+8 = 97(17) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 21+6 = 27(27) 1/0 1\n",
      "Epoch 170. Train Loss: 2.2318998e-06, Test Loss : 0.0018041984\n",
      "Epoch 171. Train Loss: 2.1953035e-06, Test Loss : 0.0018844813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172. Train Loss: 2.1420558e-06, Test Loss : 0.0018470923\n",
      "Epoch 173. Train Loss: 2.0954724e-06, Test Loss : 0.001825155\n",
      "Epoch 174. Train Loss: 2.1623584e-06, Test Loss : 0.001800246\n",
      "Epoch 175. Train Loss: 2.1547653e-06, Test Loss : 0.0018029095\n",
      "Epoch 176. Train Loss: 2.1017363e-06, Test Loss : 0.0018171541\n",
      "Epoch 177. Train Loss: 2.0645996e-06, Test Loss : 0.0018283458\n",
      "Epoch 178. Train Loss: 2.0615996e-06, Test Loss : 0.001770859\n",
      "Epoch 179. Train Loss: 2.0395282e-06, Test Loss : 0.0018300212\n",
      "\u001b[92m☑\u001b[0m 454+27 = 481(481) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+7 = 8(7) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+8 = 14(14) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 69+100 = 169(169) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 58+3 = 61(61) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+197 = 199(199) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+72 = 75(75) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 589+2 = 691(591) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 785+32 = 817(817) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+981 = 983(983) 1/0 1\n",
      "Epoch 180. Train Loss: 2.0728671e-06, Test Loss : 0.0018110892\n",
      "Epoch 181. Train Loss: 2.0122313e-06, Test Loss : 0.0021244537\n",
      "Epoch 182. Train Loss: 2.0436748e-06, Test Loss : 0.0017779007\n",
      "Epoch 183. Train Loss: 1.98998e-06, Test Loss : 0.0017872738\n",
      "Epoch 184. Train Loss: 1.91409e-06, Test Loss : 0.001910621\n",
      "Epoch 185. Train Loss: 1.954553e-06, Test Loss : 0.0018119967\n",
      "Epoch 186. Train Loss: 1.9420086e-06, Test Loss : 0.0019811583\n",
      "Epoch 187. Train Loss: 1.9111908e-06, Test Loss : 0.001834609\n",
      "Epoch 188. Train Loss: 1.9374352e-06, Test Loss : 0.0019548214\n",
      "Epoch 189. Train Loss: 1.8659972e-06, Test Loss : 0.0017946677\n",
      "\u001b[92m☑\u001b[0m 958+77 = 1035(1035) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 371+9 = 370(380) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 775+360 = 1135(1135) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 256+378 = 634(634) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+28 = 29(29) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 54+9 = 643(63) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 50+3 = 53(53) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 28+169 = 197(197) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+9 = 90(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 384+3 = 387(387) 1/0 1\n",
      "Epoch 190. Train Loss: 1.8947533e-06, Test Loss : 0.0018093545\n",
      "Epoch 191. Train Loss: 1.8665273e-06, Test Loss : 0.0018116779\n",
      "Epoch 192. Train Loss: 1.896319e-06, Test Loss : 0.001808937\n",
      "Epoch 193. Train Loss: 1.8408401e-06, Test Loss : 0.0017735673\n",
      "Epoch 194. Train Loss: 1.8463014e-06, Test Loss : 0.0018094562\n",
      "Epoch 195. Train Loss: 1.8098385e-06, Test Loss : 0.0017942579\n",
      "Epoch 196. Train Loss: 1.7822666e-06, Test Loss : 0.0018176768\n",
      "Epoch 197. Train Loss: 1.8371234e-06, Test Loss : 0.0018082209\n",
      "Epoch 198. Train Loss: 1.797962e-06, Test Loss : 0.001954653\n",
      "Epoch 199. Train Loss: 1.7590518e-06, Test Loss : 0.0018986737\n",
      "\u001b[91m☒\u001b[0m 90+9 = 108(99) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+831 = 837(837) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 927+859 = 1786(1786) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+2 = 4(4) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 830+8 = 838(838) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+944 = 951(951) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 29+85 = 114(114) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 156+8 = 164(164) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+14 = 23(23) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+89 = 97(96) 1/0 0\n",
      "Epoch 200. Train Loss: 1.7511651e-06, Test Loss : 0.0017797468\n"
     ]
    }
   ],
   "source": [
    "epochs = 201\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p = model.calculation(q[i], char_indices, indices_char).strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) 1/0 {}\".format(q[i], p, y[i], str(iscorr) ))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
