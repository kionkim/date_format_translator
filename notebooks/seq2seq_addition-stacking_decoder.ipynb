{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, enc_layer, dec_layer = 1, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_layer = enc_layer\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTM(hidden_size = n_hidden, num_layers = enc_layer, layout = 'NTC')\n",
    "            self.decoder_0 = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder_1 = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        # API says: num_layers, batch_size, num_hidden\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        begin_state = self.encoder.begin_state(batch_size = self.batch_size, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(inputs, begin_state) # h, c: n_layer * batch_size * n_hidden\n",
    "        # Pick the hidden states and cell states at the last time step in the second layer\n",
    "        next_h_0 = h[0] # batch_size * n_hidden\n",
    "        next_c_0 = c[0] # batch_size * n_hidden\n",
    "        next_h_1 = h[1] # batch_size * n_hidden\n",
    "        next_c_1 = c[1] # batch_size * n_hidden\n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h_0, next_c_0) = self.decoder_0(outputs[:, i, :], [next_h_0, next_c_0],)\n",
    "            deout, (next_h_1, next_c_1) = self.decoder_1(deout, [next_h_1, next_c_1],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculation(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        begin_state = self.encoder.begin_state(batch_size = 1, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(X, begin_state)\n",
    "        next_h_0 = h[0] # batch_size * n_hidden\n",
    "        next_c_0 = c[0] # batch_size * n_hidden\n",
    "        next_h_1 = h[1] # batch_size * n_hidden\n",
    "        next_c_1 = c[1] # batch_size * n_hidden\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h_0, next_c_0) = self.decoder_0(deout, [next_h_0, next_c_0],)\n",
    "            deout, (next_h_1, next_c_1) = self.decoder_1(deout, [next_h_1, next_c_1],)\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "        return ret_seq.strip('E').strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14, 2)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTM(None -> 300, NTC, num_layers=2)\n",
      "  (decoder_0): LSTMCell(None -> 1200)\n",
      "  (decoder_1): LSTMCell(None -> 1200)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 4+83 = 103(87) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 32+6 = 132(38) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 23+35 = 338(58) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 804+140 = 1003(944) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 501+37 = 550(538) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 48+6 = 153(54) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+2 = 122(3) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 307+595 = 1003(902) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 36+3 = 133(39) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 4+4 = 553(8) 1/0 0\n",
      "Epoch 0. Train Loss: 1.1887326, Test Loss : 1.137922\n",
      "Epoch 1. Train Loss: 1.1229072, Test Loss : 1.1939143\n",
      "Epoch 2. Train Loss: 1.0980244, Test Loss : 1.1030667\n",
      "Epoch 3. Train Loss: 1.0271647, Test Loss : 1.0102959\n",
      "Epoch 4. Train Loss: 0.91535574, Test Loss : 0.85927093\n",
      "Epoch 5. Train Loss: 0.836573, Test Loss : 0.8067001\n",
      "Epoch 6. Train Loss: 0.76906264, Test Loss : 0.72716874\n",
      "Epoch 7. Train Loss: 0.68128014, Test Loss : 0.6523584\n",
      "Epoch 8. Train Loss: 0.5845852, Test Loss : 0.5454272\n",
      "Epoch 9. Train Loss: 0.50449175, Test Loss : 0.52240574\n",
      "\u001b[91m☒\u001b[0m 580+46 = 625(626) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+6 = 133(12) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+5 = 10(10) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 116+50 = 167(166) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 86+9 = 174(95) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 554+51 = 605(605) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 154+0 = 155(154) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+660 = 666(666) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 464+649 = 1013(1113) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 530+169 = 799(699) 1/0 0\n",
      "Epoch 10. Train Loss: 0.38444602, Test Loss : 0.3038127\n",
      "Epoch 11. Train Loss: 0.24645762, Test Loss : 0.26029518\n",
      "Epoch 12. Train Loss: 0.16490413, Test Loss : 0.14394487\n",
      "Epoch 13. Train Loss: 0.11600388, Test Loss : 0.095136955\n",
      "Epoch 14. Train Loss: 0.08584781, Test Loss : 0.21599385\n",
      "Epoch 15. Train Loss: 0.07301192, Test Loss : 0.037842743\n",
      "Epoch 16. Train Loss: 0.061347116, Test Loss : 0.02981535\n",
      "Epoch 17. Train Loss: 0.051965907, Test Loss : 0.03176847\n",
      "Epoch 18. Train Loss: 0.04148146, Test Loss : 0.021222396\n",
      "Epoch 19. Train Loss: 0.042022854, Test Loss : 0.09817979\n",
      "\u001b[92m☑\u001b[0m 25+881 = 906(906) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 214+209 = 423(423) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+460 = 467(467) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 34+8 = 42(42) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+49 = 56(56) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 293+5 = 298(298) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+158 = 165(165) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+7 = 145(14) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 62+29 = 92(91) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 9+805 = 814(814) 1/0 1\n",
      "Epoch 20. Train Loss: 0.032434475, Test Loss : 0.016998094\n",
      "Epoch 21. Train Loss: 0.030670257, Test Loss : 0.017694907\n",
      "Epoch 22. Train Loss: 0.030255698, Test Loss : 0.0125239\n",
      "Epoch 23. Train Loss: 0.025987674, Test Loss : 0.014017033\n",
      "Epoch 24. Train Loss: 0.020362664, Test Loss : 0.019292135\n",
      "Epoch 25. Train Loss: 0.023446439, Test Loss : 0.012114417\n",
      "Epoch 26. Train Loss: 0.018227797, Test Loss : 0.013686871\n",
      "Epoch 27. Train Loss: 0.018378137, Test Loss : 0.17333822\n",
      "Epoch 28. Train Loss: 0.013710872, Test Loss : 0.012312712\n",
      "Epoch 29. Train Loss: 0.01791627, Test Loss : 0.005768542\n",
      "\u001b[92m☑\u001b[0m 563+402 = 965(965) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 832+87 = 919(919) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 76+1 = 78(77) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+38 = 44(44) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 12+335 = 347(347) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+514 = 517(517) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 197+877 = 1074(1074) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+879 = 886(886) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 76+60 = 126(136) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 490+897 = 1387(1387) 1/0 1\n",
      "Epoch 30. Train Loss: 0.01346127, Test Loss : 0.033232313\n",
      "Epoch 31. Train Loss: 0.012854595, Test Loss : 0.0069492785\n",
      "Epoch 32. Train Loss: 0.015064153, Test Loss : 0.0060995882\n",
      "Epoch 33. Train Loss: 0.008481937, Test Loss : 0.005829973\n",
      "Epoch 34. Train Loss: 0.013608304, Test Loss : 0.0074663474\n",
      "Epoch 35. Train Loss: 0.009697306, Test Loss : 0.01319159\n",
      "Epoch 36. Train Loss: 0.011339306, Test Loss : 0.0060141697\n",
      "Epoch 37. Train Loss: 0.008854091, Test Loss : 0.008366672\n",
      "Epoch 38. Train Loss: 0.0094720125, Test Loss : 0.006538824\n",
      "Epoch 39. Train Loss: 0.0083768945, Test Loss : 0.007816007\n",
      "\u001b[91m☒\u001b[0m 5+2 = 6(7) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+89 = 899(89) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 169+77 = 246(246) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 9+3 = 32(12) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 49+939 = 988(988) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+29 = 32(33) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 35+621 = 656(656) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 309+729 = 1038(1038) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 242+501 = 743(743) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+25 = 33(32) 1/0 0\n",
      "Epoch 40. Train Loss: 0.010146175, Test Loss : 0.0064843474\n",
      "Epoch 41. Train Loss: 0.008472879, Test Loss : 0.006514254\n",
      "Epoch 42. Train Loss: 0.0072423606, Test Loss : 0.00645433\n",
      "Epoch 43. Train Loss: 0.0060464083, Test Loss : 0.002899733\n",
      "Epoch 44. Train Loss: 0.007535458, Test Loss : 0.008943251\n",
      "Epoch 45. Train Loss: 0.0054226364, Test Loss : 0.0042783357\n",
      "Epoch 46. Train Loss: 0.007541738, Test Loss : 0.0039597135\n",
      "Epoch 47. Train Loss: 0.005862248, Test Loss : 0.003036323\n",
      "Epoch 48. Train Loss: 0.006672132, Test Loss : 0.0029513708\n",
      "Epoch 49. Train Loss: 0.0050532934, Test Loss : 0.0029538479\n",
      "\u001b[92m☑\u001b[0m 642+9 = 651(651) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+189 = 194(194) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 218+8 = 226(226) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 577+430 = 1007(1007) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 70+2 = 83(72) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+3 = 32(12) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 82+1 = 83(83) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 66+2 = 69(68) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 72+215 = 287(287) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 54+954 = 1008(1008) 1/0 1\n",
      "Epoch 50. Train Loss: 0.00032553167, Test Loss : 0.002272663\n",
      "Epoch 51. Train Loss: 8.5656444e-05, Test Loss : 0.0019175526\n",
      "Epoch 52. Train Loss: 5.7248104e-05, Test Loss : 0.0019809962\n",
      "Epoch 53. Train Loss: 4.5667508e-05, Test Loss : 0.0016253801\n",
      "Epoch 54. Train Loss: 3.8666127e-05, Test Loss : 0.00158116\n",
      "Epoch 55. Train Loss: 3.3837303e-05, Test Loss : 0.0014716352\n",
      "Epoch 56. Train Loss: 2.990936e-05, Test Loss : 0.0016430328\n",
      "Epoch 57. Train Loss: 2.6972983e-05, Test Loss : 0.0015222251\n",
      "Epoch 58. Train Loss: 2.4768127e-05, Test Loss : 0.0014327972\n",
      "Epoch 59. Train Loss: 2.2749004e-05, Test Loss : 0.0014526544\n",
      "\u001b[92m☑\u001b[0m 504+711 = 1215(1215) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+319 = 325(325) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 55+9 = 64(64) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 33+85 = 118(118) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 870+54 = 924(924) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 230+9 = 239(239) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 99+128 = 227(227) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 459+275 = 734(734) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+316 = 324(324) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 890+89 = 979(979) 1/0 1\n",
      "Epoch 60. Train Loss: 2.1233362e-05, Test Loss : 0.0015449955\n",
      "Epoch 61. Train Loss: 1.9804453e-05, Test Loss : 0.0014205116\n",
      "Epoch 62. Train Loss: 1.8419783e-05, Test Loss : 0.0013514713\n",
      "Epoch 63. Train Loss: 1.7357828e-05, Test Loss : 0.0013983867\n",
      "Epoch 64. Train Loss: 1.626275e-05, Test Loss : 0.0013672495\n",
      "Epoch 65. Train Loss: 1.5493728e-05, Test Loss : 0.0013514905\n",
      "Epoch 66. Train Loss: 1.4768667e-05, Test Loss : 0.0013704449\n",
      "Epoch 67. Train Loss: 1.4102202e-05, Test Loss : 0.0013380062\n",
      "Epoch 68. Train Loss: 1.3361859e-05, Test Loss : 0.0012887801\n",
      "Epoch 69. Train Loss: 1.2854342e-05, Test Loss : 0.001329052\n",
      "\u001b[92m☑\u001b[0m 4+83 = 87(87) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+97 = 116(116) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 62+3 = 65(65) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+56 = 59(59) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+55 = 55(56) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 21+2 = 34(23) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 2+48 = 50(50) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+1 = 2(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 253+817 = 1070(1070) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+353 = 361(361) 1/0 1\n",
      "Epoch 70. Train Loss: 1.2461145e-05, Test Loss : 0.001286357\n",
      "Epoch 71. Train Loss: 1.17810305e-05, Test Loss : 0.0013188665\n",
      "Epoch 72. Train Loss: 1.1299099e-05, Test Loss : 0.0012841312\n",
      "Epoch 73. Train Loss: 1.0751296e-05, Test Loss : 0.0012843953\n",
      "Epoch 74. Train Loss: 1.0431547e-05, Test Loss : 0.0013033577\n",
      "Epoch 75. Train Loss: 1.014928e-05, Test Loss : 0.0013046669\n",
      "Epoch 76. Train Loss: 9.949233e-06, Test Loss : 0.0012399416\n",
      "Epoch 77. Train Loss: 9.495529e-06, Test Loss : 0.0012167224\n",
      "Epoch 78. Train Loss: 9.216382e-06, Test Loss : 0.0012697241\n",
      "Epoch 79. Train Loss: 8.990777e-06, Test Loss : 0.0012437659\n",
      "\u001b[92m☑\u001b[0m 5+177 = 182(182) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 606+8 = 614(614) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 373+902 = 1275(1275) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 753+7 = 750(760) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 7+8 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+933 = 935(935) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 492+56 = 548(548) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 75+509 = 584(584) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 876+5 = 881(881) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 95+26 = 121(121) 1/0 1\n",
      "Epoch 80. Train Loss: 8.878245e-06, Test Loss : 0.0012899407\n",
      "Epoch 81. Train Loss: 8.464382e-06, Test Loss : 0.0013079664\n",
      "Epoch 82. Train Loss: 8.481973e-06, Test Loss : 0.0011872407\n",
      "Epoch 83. Train Loss: 8.12419e-06, Test Loss : 0.0011683021\n",
      "Epoch 84. Train Loss: 7.843218e-06, Test Loss : 0.0012076346\n",
      "Epoch 85. Train Loss: 7.6807355e-06, Test Loss : 0.0011676219\n",
      "Epoch 86. Train Loss: 7.374585e-06, Test Loss : 0.0011397547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87. Train Loss: 7.3070696e-06, Test Loss : 0.0011937881\n",
      "Epoch 88. Train Loss: 7.1502695e-06, Test Loss : 0.0011608541\n",
      "Epoch 89. Train Loss: 6.8019885e-06, Test Loss : 0.0011796046\n",
      "\u001b[92m☑\u001b[0m 7+41 = 48(48) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+5 = 13(13) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+53 = 62(62) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 86+9 = 175(95) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+12 = 17(17) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+2 = 11(10) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+7 = 134(13) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 711+717 = 1428(1428) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+450 = 458(458) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 63+44 = 107(107) 1/0 1\n",
      "Epoch 90. Train Loss: 6.6332636e-06, Test Loss : 0.001151774\n",
      "Epoch 91. Train Loss: 6.4999517e-06, Test Loss : 0.0012712192\n",
      "Epoch 92. Train Loss: 6.4856195e-06, Test Loss : 0.0011744685\n",
      "Epoch 93. Train Loss: 6.3154544e-06, Test Loss : 0.0011834592\n",
      "Epoch 94. Train Loss: 6.149737e-06, Test Loss : 0.0011855698\n",
      "Epoch 95. Train Loss: 6.130489e-06, Test Loss : 0.0012341099\n",
      "Epoch 96. Train Loss: 5.953523e-06, Test Loss : 0.0011711314\n",
      "Epoch 97. Train Loss: 5.920632e-06, Test Loss : 0.0011936727\n",
      "Epoch 98. Train Loss: 5.6574936e-06, Test Loss : 0.001177714\n",
      "Epoch 99. Train Loss: 5.7034604e-06, Test Loss : 0.0011701501\n",
      "\u001b[92m☑\u001b[0m 606+95 = 701(701) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 879+4 = 883(883) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+30 = 30(30) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+8 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 46+910 = 956(956) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+273 = 273(273) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+7 = 14(14) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+8 = 10(11) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 1+359 = 360(360) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 866+0 = 866(866) 1/0 1\n",
      "Epoch 100. Train Loss: 5.5311452e-06, Test Loss : 0.0011204731\n",
      "Epoch 101. Train Loss: 5.4091156e-06, Test Loss : 0.0012111126\n",
      "Epoch 102. Train Loss: 5.2981104e-06, Test Loss : 0.0011803138\n",
      "Epoch 103. Train Loss: 5.0877165e-06, Test Loss : 0.0011582908\n",
      "Epoch 104. Train Loss: 5.0699655e-06, Test Loss : 0.0011594883\n",
      "Epoch 105. Train Loss: 5.002895e-06, Test Loss : 0.0011378605\n",
      "Epoch 106. Train Loss: 4.853339e-06, Test Loss : 0.0011945966\n",
      "Epoch 107. Train Loss: 4.8373795e-06, Test Loss : 0.0011402521\n",
      "Epoch 108. Train Loss: 4.7168637e-06, Test Loss : 0.0011560281\n",
      "Epoch 109. Train Loss: 4.686824e-06, Test Loss : 0.0012047712\n",
      "\u001b[91m☒\u001b[0m 2+9 = 100(11) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 10+24 = 34(34) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 29+72 = 101(101) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+31 = 36(36) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+354 = 362(362) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 30+830 = 860(860) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+8 = 13(13) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 569+40 = 609(609) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+94 = 97(97) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 245+821 = 1066(1066) 1/0 1\n",
      "Epoch 110. Train Loss: 4.65843e-06, Test Loss : 0.0011647505\n",
      "Epoch 111. Train Loss: 4.5200018e-06, Test Loss : 0.0011353351\n",
      "Epoch 112. Train Loss: 4.4879416e-06, Test Loss : 0.0011216737\n",
      "Epoch 113. Train Loss: 4.36148e-06, Test Loss : 0.0011060522\n",
      "Epoch 114. Train Loss: 4.3365776e-06, Test Loss : 0.0011545423\n",
      "Epoch 115. Train Loss: 4.2639035e-06, Test Loss : 0.0011077801\n",
      "Epoch 116. Train Loss: 4.2102497e-06, Test Loss : 0.0011188483\n",
      "Epoch 117. Train Loss: 4.1588687e-06, Test Loss : 0.0011923196\n",
      "Epoch 118. Train Loss: 4.1370604e-06, Test Loss : 0.0011373945\n",
      "Epoch 119. Train Loss: 4.0361447e-06, Test Loss : 0.0011232599\n",
      "\u001b[92m☑\u001b[0m 5+393 = 398(398) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 696+83 = 779(779) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+39 = 48(48) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 226+8 = 234(234) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 886+5 = 891(891) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 715+74 = 789(789) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 255+481 = 736(736) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+715 = 720(720) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+145 = 146(146) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 622+9 = 731(631) 1/0 0\n",
      "Epoch 120. Train Loss: 3.963074e-06, Test Loss : 0.0011104892\n",
      "Epoch 121. Train Loss: 3.889685e-06, Test Loss : 0.0011380914\n",
      "Epoch 122. Train Loss: 3.8184266e-06, Test Loss : 0.0011135645\n",
      "Epoch 123. Train Loss: 3.7711231e-06, Test Loss : 0.0011195506\n",
      "Epoch 124. Train Loss: 3.7750617e-06, Test Loss : 0.0011028424\n",
      "Epoch 125. Train Loss: 3.637824e-06, Test Loss : 0.0011967903\n",
      "Epoch 126. Train Loss: 3.662629e-06, Test Loss : 0.001222396\n",
      "Epoch 127. Train Loss: 3.6091217e-06, Test Loss : 0.0010861601\n",
      "Epoch 128. Train Loss: 3.6286192e-06, Test Loss : 0.0011126318\n",
      "Epoch 129. Train Loss: 3.5451355e-06, Test Loss : 0.00108554\n",
      "\u001b[92m☑\u001b[0m 4+333 = 337(337) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 233+630 = 863(863) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 10+589 = 599(599) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 513+74 = 587(587) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 717+74 = 791(791) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 907+5 = 912(912) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 345+690 = 1035(1035) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 408+444 = 852(852) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+5 = 10(10) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+7 = 134(13) 1/0 0\n",
      "Epoch 130. Train Loss: 3.514736e-06, Test Loss : 0.0010883297\n",
      "Epoch 131. Train Loss: 3.4672148e-06, Test Loss : 0.0010697107\n",
      "Epoch 132. Train Loss: 3.3828096e-06, Test Loss : 0.001071831\n",
      "Epoch 133. Train Loss: 3.3383303e-06, Test Loss : 0.0011084239\n",
      "Epoch 134. Train Loss: 3.3718518e-06, Test Loss : 0.0011368945\n",
      "Epoch 135. Train Loss: 3.2940363e-06, Test Loss : 0.0010918669\n",
      "Epoch 136. Train Loss: 3.311621e-06, Test Loss : 0.0010800423\n",
      "Epoch 137. Train Loss: 3.2487674e-06, Test Loss : 0.0011562724\n",
      "Epoch 138. Train Loss: 3.2188075e-06, Test Loss : 0.0010694609\n",
      "Epoch 139. Train Loss: 3.1251402e-06, Test Loss : 0.0010784337\n",
      "\u001b[92m☑\u001b[0m 333+759 = 1092(1092) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+154 = 162(162) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+84 = 89(89) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 77+265 = 342(342) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+82 = 90(90) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 983+8 = 1001(991) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 774+547 = 1321(1321) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+250 = 255(255) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 305+8 = 313(313) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 73+1 = 74(74) 1/0 1\n",
      "Epoch 140. Train Loss: 3.0990175e-06, Test Loss : 0.0010756253\n",
      "Epoch 141. Train Loss: 3.0289834e-06, Test Loss : 0.0011447773\n",
      "Epoch 142. Train Loss: 3.0375807e-06, Test Loss : 0.0010828815\n",
      "Epoch 143. Train Loss: 3.0526694e-06, Test Loss : 0.0010891376\n",
      "Epoch 144. Train Loss: 2.983216e-06, Test Loss : 0.0010535067\n",
      "Epoch 145. Train Loss: 2.9063042e-06, Test Loss : 0.0011182905\n",
      "Epoch 146. Train Loss: 2.8948596e-06, Test Loss : 0.0010762528\n",
      "Epoch 147. Train Loss: 2.8961142e-06, Test Loss : 0.0011137857\n",
      "Epoch 148. Train Loss: 2.8457691e-06, Test Loss : 0.0010717034\n",
      "Epoch 149. Train Loss: 2.8225556e-06, Test Loss : 0.0010800592\n",
      "\u001b[92m☑\u001b[0m 97+559 = 656(656) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+30 = 30(30) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+7 = 134(13) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 13+96 = 109(109) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 17+52 = 69(69) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 317+3 = 320(320) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 866+977 = 1843(1843) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+29 = 29(29) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 77+671 = 748(748) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 204+87 = 291(291) 1/0 1\n",
      "Epoch 150. Train Loss: 2.8173924e-06, Test Loss : 0.0010862367\n",
      "Epoch 151. Train Loss: 2.813596e-06, Test Loss : 0.001081136\n",
      "Epoch 152. Train Loss: 2.7124524e-06, Test Loss : 0.0010444445\n",
      "Epoch 153. Train Loss: 2.7022036e-06, Test Loss : 0.0011144271\n",
      "Epoch 154. Train Loss: 2.6388889e-06, Test Loss : 0.001133723\n",
      "Epoch 155. Train Loss: 2.7544468e-06, Test Loss : 0.0011000662\n",
      "Epoch 156. Train Loss: 2.661877e-06, Test Loss : 0.0010456725\n",
      "Epoch 157. Train Loss: 2.6368796e-06, Test Loss : 0.0010565622\n",
      "Epoch 158. Train Loss: 2.6033297e-06, Test Loss : 0.0010672493\n",
      "Epoch 159. Train Loss: 2.5387342e-06, Test Loss : 0.0011274503\n",
      "\u001b[91m☒\u001b[0m 71+1 = 73(72) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 2+75 = 77(77) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+27 = 30(30) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 873+8 = 871(881) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 92+3 = 95(95) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 898+1 = 909(899) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 0+38 = 38(38) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+463 = 472(472) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+74 = 79(79) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 564+64 = 628(628) 1/0 1\n",
      "Epoch 160. Train Loss: 2.5549584e-06, Test Loss : 0.0010981712\n",
      "Epoch 161. Train Loss: 2.5822364e-06, Test Loss : 0.0010398149\n",
      "Epoch 162. Train Loss: 2.5491884e-06, Test Loss : 0.0010271228\n",
      "Epoch 163. Train Loss: 2.4827204e-06, Test Loss : 0.001036795\n",
      "Epoch 164. Train Loss: 2.4628332e-06, Test Loss : 0.0010452521\n",
      "Epoch 165. Train Loss: 2.4248855e-06, Test Loss : 0.001034818\n",
      "Epoch 166. Train Loss: 2.438273e-06, Test Loss : 0.0011097963\n",
      "Epoch 167. Train Loss: 2.3816478e-06, Test Loss : 0.0010389483\n",
      "Epoch 168. Train Loss: 2.3495907e-06, Test Loss : 0.001040172\n",
      "Epoch 169. Train Loss: 2.3755874e-06, Test Loss : 0.0010503104\n",
      "\u001b[92m☑\u001b[0m 640+758 = 1398(1398) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+726 = 728(728) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 109+7 = 116(116) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 80+221 = 301(301) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 53+14 = 67(67) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 65+212 = 277(277) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+5 = 5(6) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 4+67 = 71(71) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 46+2 = 58(48) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+59 = 688(68) 1/0 0\n",
      "Epoch 170. Train Loss: 2.2996335e-06, Test Loss : 0.0010929467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171. Train Loss: 2.3104656e-06, Test Loss : 0.0010896982\n",
      "Epoch 172. Train Loss: 2.3356392e-06, Test Loss : 0.0010612385\n",
      "Epoch 173. Train Loss: 2.2933402e-06, Test Loss : 0.0010352186\n",
      "Epoch 174. Train Loss: 2.2688014e-06, Test Loss : 0.0010203301\n",
      "Epoch 175. Train Loss: 2.2571119e-06, Test Loss : 0.001018396\n",
      "Epoch 176. Train Loss: 2.1932153e-06, Test Loss : 0.0010605085\n",
      "Epoch 177. Train Loss: 2.1797202e-06, Test Loss : 0.0010207925\n",
      "Epoch 178. Train Loss: 2.1409842e-06, Test Loss : 0.001036236\n",
      "Epoch 179. Train Loss: 2.1663159e-06, Test Loss : 0.0010475998\n",
      "\u001b[92m☑\u001b[0m 398+68 = 466(466) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+669 = 673(673) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+847 = 855(855) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 52+26 = 78(78) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 425+7 = 432(432) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 992+7 = 1000(999) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+931 = 939(939) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 44+678 = 722(722) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+59 = 63(63) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+4 = 7(7) 1/0 1\n",
      "Epoch 180. Train Loss: 2.1899762e-06, Test Loss : 0.0010465644\n",
      "Epoch 181. Train Loss: 2.1507333e-06, Test Loss : 0.0010328765\n",
      "Epoch 182. Train Loss: 2.1278631e-06, Test Loss : 0.0010787027\n",
      "Epoch 183. Train Loss: 2.0864777e-06, Test Loss : 0.0010409518\n",
      "Epoch 184. Train Loss: 2.0949242e-06, Test Loss : 0.0010334819\n",
      "Epoch 185. Train Loss: 2.077078e-06, Test Loss : 0.001013837\n",
      "Epoch 186. Train Loss: 2.0809903e-06, Test Loss : 0.0010566822\n",
      "Epoch 187. Train Loss: 2.035754e-06, Test Loss : 0.001031414\n",
      "Epoch 188. Train Loss: 2.014049e-06, Test Loss : 0.0010058568\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[00:15:38] src/storage/./pooled_storage_manager.h:119: cudaMalloc failed: out of memory\n\nStack trace returned 10 entries:\n[bt] (0) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x34aee2) [0x7f077e35aee2]\n[bt] (1) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x34b4a8) [0x7f077e35b4a8]\n[bt] (2) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x3533a4f) [0x7f0781543a4f]\n[bt] (3) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x353853c) [0x7f078154853c]\n[bt] (4) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e59ca0) [0x7f0780e69ca0]\n[bt] (5) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e59f17) [0x7f0780e69f17]\n[bt] (6) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e5a319) [0x7f0780e6a319]\n[bt] (7) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc09f2) [0x7f0780dd09f2]\n[bt] (8) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc09f2) [0x7f0780dd09f2]\n[bt] (9) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc09f2) [0x7f0780dd09f2]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8b0938f20b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \"\"\"\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [00:15:38] src/storage/./pooled_storage_manager.h:119: cudaMalloc failed: out of memory\n\nStack trace returned 10 entries:\n[bt] (0) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x34aee2) [0x7f077e35aee2]\n[bt] (1) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x34b4a8) [0x7f077e35b4a8]\n[bt] (2) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x3533a4f) [0x7f0781543a4f]\n[bt] (3) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x353853c) [0x7f078154853c]\n[bt] (4) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e59ca0) [0x7f0780e69ca0]\n[bt] (5) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e59f17) [0x7f0780e69f17]\n[bt] (6) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e5a319) [0x7f0780e6a319]\n[bt] (7) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc09f2) [0x7f0780dd09f2]\n[bt] (8) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc09f2) [0x7f0780dd09f2]\n[bt] (9) /opt/venv/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc09f2) [0x7f0780dd09f2]\n\n"
     ]
    }
   ],
   "source": [
    "epochs = 201\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p = model.calculation(q[i], char_indices, indices_char).strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) 1/0 {}\".format(q[i], p, y[i], str(iscorr) ))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
